<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>涂蓝</title>
  
  
  <link href="https://tulancn.github.io/atom.xml" rel="self"/>
  
  <link href="https://tulancn.github.io/"/>
  <updated>2025-04-08T13:05:08.598Z</updated>
  <id>https://tulancn.github.io/</id>
  
  <author>
    <name>Tulan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>redis</title>
    <link href="https://tulancn.github.io/2025/04/08/study/redis/"/>
    <id>https://tulancn.github.io/2025/04/08/study/redis/</id>
    <published>2025-04-08T08:55:35.000Z</published>
    <updated>2025-04-08T13:05:08.598Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis基础概念">Redis基础概念</h2><p>Redis是一个开源的、基于内存的数据结构存储系统，常被用作数据库、缓存和消息中间件。</p><p>它支持多种数据结构，包括字符串（String）、哈希（Hash）、列表（List）、集合（Set）和有序集合（Sorted Set）等。每种数据结构都有其独特的操作命令和适用场景。</p><p>redis是个k-v数据库，在redis内部维护了一个哈希表。当有查询来的时候，会先查全局的表获取键值，然后基于键值的类型进行后续处理。</p><p>String就理解为用了一个全局的哈希表。</p><p>Hash就是在全局的哈希表中再建了一个哈希表，比如专门建立一个user表用于存用户信息。这种表查询的时候会有两次哈希，第一次是查键，第二次是查具体的表。</p><p>Set、List也是类似，这里省略了。</p><h2 id="Redis常用命令">Redis常用命令</h2><ul><li><strong>字符串操作</strong>：<code>SET</code> 用于设置键值对，<code>GET</code> 用于获取指定键的值，<code>INCR</code> 用于将键的值加 1（键值需为整数）。</li><li><strong>哈希操作</strong>：<code>HSET</code> 在哈希表中设置字段值，<code>HGET</code> 获取哈希表中指定字段的值，<code>HGETALL</code> 获取哈希表中所有字段和值。</li><li><strong>列表操作</strong>：<code>LPUSH</code> 从列表头部插入元素，<code>RPUSH</code> 从列表尾部插入元素，<code>LPOP</code> 移除并返回列表的第一个元素，<code>RPOP</code> 移除并返回列表的最后一个元素，<code>LRANGE</code> 返回列表中指定区间内的元素。</li><li><strong>集合操作</strong>：<code>SADD</code> 添加元素到集合，<code>SMEMBERS</code> 返回集合中的所有元素，<code>SISMEMBER</code> 判断元素是否存在于集合中，<code>SREM</code> 移除集合中的元素，还支持集合的交集（<code>SINTER</code>）、并集（<code>SUNION</code>）、差集（<code>SDIFF</code>）等运算。</li><li><strong>有序集合操作</strong>：<code>ZADD</code> 添加成员到有序集合并指定分数，<code>ZRANGE</code> 返回有序集合中指定区间内的成员（按分数从小到大排序），<code>ZREM</code> 移除有序集合中的成员。</li><li><strong>键操作</strong>：<code>DEL</code> 删除一个或多个键，<code>KEYS</code> 查找所有符合给定模式的键，<code>EXISTS</code> 检查给定键是否存在。</li></ul><h2 id="Redis数据结构使用">Redis数据结构使用</h2><ul><li><strong>集合（Set）</strong>：适用于去重、判断元素是否存在以及集合运算等场景，例如存储用户标签、共同好友等。使用Python的<code>redis-py</code>库操作集合时，可通过<code>sadd</code>、<code>smembers</code>等方法实现添加元素、获取所有元素等操作。</li><li><strong>列表（List）</strong>：可用于实现队列（先进先出，FIFO）、栈（先进后出，LIFO）等数据结构，如消息队列、任务队列等。使用<code>redis-py</code>库时，<code>lpush</code>、<code>rpush</code>等方法可实现列表元素的插入操作。</li></ul><h2 id="Redis本地连接方式">Redis本地连接方式</h2><p>远程连接走的TCP，这里省略。</p><ul><li><strong>TCP/IP连接</strong>：即使Redis服务器和应用程序在同一台机器上，使用TCP/IP连接时，数据会经过本地的网络协议栈，通过回环地址进行传输。在Python中，使用<code>redis-py</code>库通过TCP/IP连接本地Redis服务器时，可指定<code>host</code>和<code>port</code>参数。</li><li><strong>本地套接字（Unix Domain Socket）连接</strong>：应用程序与Redis服务器通过本地文件系统中的套接字文件进行通信，不经过网络协议栈，效率较高。在Python中，使用<code>redis-py</code>库可通过指定<code>unix_socket_path</code>参数来使用本地套接字连接Redis。</li></ul><p>本地的套接字连接还是要走网络的那一套，建立连接、监听之类的，所以性能上比JNI要低。</p><h2 id="Redis性能优化技巧">Redis性能优化技巧</h2><ul><li><strong>配置优化</strong>：合理设置<code>maxmemory</code>参数和内存淘汰策略；根据业务需求选择合适的持久化方式（RDB或AOF），并调整相关参数。</li><li><strong>数据结构优化</strong>：根据业务场景选择合适的数据结构，优化数据结构设计以减少内存占用。</li><li><strong>客户端优化</strong>：使用连接池管理客户端连接，减少网络往返次数，控制请求数据量。</li><li><strong>服务器优化</strong>：将Redis服务器运行在单核CPU上，绑定特定CPU核心；使用高性能存储设备，优化操作系统I/O调度算法。</li><li><strong>监控与优化</strong>：使用<code>INFO</code>、<code>MONITOR</code>等命令监控性能指标，分析并优化查询语句。</li></ul><h2 id="持久化">持久化</h2><p>RDB，固定时间把所有数据写入本地文件。</p><p>AOF，每次写操作都把数据写入文件。</p><p>可以使用AOF的特殊参数，来实现每隔多少时间把写操作写入文件中。</p><p>这种方式类似MySQL的binlog。</p><h2 id="Redis集群方案">Redis集群方案</h2><ul><li><strong>主从复制</strong>：主节点处理写操作并将数据异步复制到从节点，从节点处理读操作。优点是实现读操作负载均衡，提高读性能；缺点是主节点单点故障，可能存在数据丢失风险。</li><li><strong>Sentinel（哨兵）</strong>：在主从复制基础上，引入Sentinel进程监控节点状态。当主节点故障时，自动从从节点中选举新主节点。多个Sentinel节点组成集群，通过Gossip协议通信，提高监控可靠性。但配置和管理相对复杂，故障转移可能存在短暂服务中断和数据不一致情况。</li><li><strong>Redis Cluster</strong>：采用数据分片，将键空间划分为16384个哈希槽，每个节点负责一部分哈希槽。节点间通过内部二进制协议通信，使用哨兵机制进行故障检测和转移。支持水平扩展，客户端可连接任意节点，通过<code>MOVED</code>错误重定向到正确节点。</li><li><strong>Codis</strong>：由Codis Server、Codis Proxy和ZooKeeper等组件组成。Codis Server存储数据，Codis Proxy代理客户端请求，ZooKeeper存储集群配置信息。支持动态扩展和收缩集群，对客户端透明，但依赖ZooKeeper，性能可能有损耗。</li></ul><p>主从是最基础的方案，配合AOF很好理解。能在多读少写的场景实现很有效的集群。</p><p>Sentinel机制是在主从的基础上添加了高可用的机制，能在主节点故障时自动选举出新的主节点。</p><p>Redis Cluster则是数据分片存储的方案，类似分库分表，实现上也很像。</p><h2 id="相关通信协议">相关通信协议</h2><ul><li><strong>Gossip协议</strong>：是一种分布式算法，用于在节点之间传递信息。具有去中心化、简单、扩展性好、容错性强、性能高效等特点。通过节点间随机的信息交换实现最终一致性，常见的通信方式有Push、Pull、Push&amp;Pull。适用于对一致性要求不是非常严格，但对系统扩展性、容错性和性能有较高要求的场景。</li><li><strong>与其他协议对比</strong>：与Raft、Paxos、Zookeeper等协议相比，在一致性保证、性能、扩展性、故障容错性、复杂性、应用场景等方面存在差异。例如，Raft和Paxos保证强一致性，Gossip协议提供最终一致性；Zookeeper是分布式协调服务，而Gossip协议主要用于信息传播和状态同步。</li></ul><h2 id="客户端从Redis-Cluster获取数据的过程">客户端从Redis Cluster获取数据的过程</h2><p>客户端先计算键的哈希槽编号，随机连接集群中的一个节点并发送获取数据的命令。若该节点负责该哈希槽，则直接返回数据；若不负责，则返回<code>MOVED</code>错误，包含目标节点信息。客户端根据错误信息重定向到正确节点再次请求数据。</p><p>客户端可维护哈希槽到节点的映射表以减少重定向开销。</p><h2 id="Redis的单线程">Redis的单线程</h2><p>Redis是个事件驱动的架构。维护了一个事件队列，线程不断从事件队列中取出事件进行处理。</p><p>比如发送了命令，那么会自动进入事件队列，上一个事件结束时才会处理下一个事件，这样保证了事务的一致性。</p><p>那么如何保证性能？</p><p>首先大部分操作都是纯内存的操作，耗时极短，用时在纳秒和微秒级别。</p><p>最耗费时间的操作往往都是IO操作，redis使用了IO多路复用的技术。没有IO多路复用的话，收到请求时，监听线程发起进行读操作，然后会阻塞，直到IO设备把数据读取到了内存中才能进行后续处理。IO多路复用的情况下，监听线程发起读操作，然后检查是否真的能读，不能则通过epoll的机制，等待内核回调。线程就直接去找下一个可以处理的任务了。</p><p>当然4.0以后，Redis也是引入了多线程机制，可以做到异步删除和异步持久化。</p>]]></content>
    
    
    <summary type="html">工作中对redis用得不多，先标记一下。</summary>
    
    
    
    <category term="学习" scheme="https://tulancn.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="缓存" scheme="https://tulancn.github.io/tags/%E7%BC%93%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>我对MCP的理解</title>
    <link href="https://tulancn.github.io/2025/03/22/study/%E6%88%91%E5%AF%B9MCP%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://tulancn.github.io/2025/03/22/study/%E6%88%91%E5%AF%B9MCP%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2025-03-22T13:30:56.000Z</published>
    <updated>2025-04-08T08:56:08.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>Model Context Portocal（MCP）真的是很火。自从这协议被提出以后，不少人觉得是一个未来的方向。</p><p>尤其是最近OpenAI也宣布要支持MCP，肉眼可见的在2025年这个协议会被人不断地提起。</p><p>2025是AI应用的大年，必然会有一批又一批的App或是软件转向拥抱AI的能力，而在这场转变中，MCP是不能绕开的一个话题。</p><h2 id="什么是MCP">什么是MCP</h2><p>在官方的介绍中，MCP被形容为大模型的type-c接口。</p><p>我觉得这概念过于抽象了，理解之后会觉得确实很有道理，但对于初学者来说这不能建立起一个直观的印象。</p><p>所以这里也想用自己的语言来组织一下我对其的理解。</p><p>目前最大的误解，就是MCP是一个标准化的Function Calling。</p><p>其实不然。</p><p>第一点，MCP是个应用层的协议，它和Function Calling完全不冲突。</p><p>第二点，函数调用是MCP中包含的能力的一部分，被称为tools。除了tools，还有其他很多功能。</p><img src="/2025/03/22/study/%E6%88%91%E5%AF%B9MCP%E7%9A%84%E7%90%86%E8%A7%A3/image-20250407162739388.png" class="" title="image-20250407162739388"><p>MCP协议还提供了prompt补全、提示词模版等能力，只不过目前客户端支持不是很到位，因此大家用的不多。</p><p>所以，为了直观理解MCP，我觉得应该先从调用方来说起。</p><p>MCP是给MCP客户端使用的，这里的MCP客户端并不等于是LLM，这点很重要。</p><p>以Claude客户端为例，我们在自己的电脑上启动Claude客户端时，客户端仅仅是提供了一个聊天界面，实际上LLM还是在云端。</p><p>这场景下，我们自己电脑上的Claude客户端是MCP Client。</p><p>假设有个工具调用触发了，我们走一下实际的调用链路。</p><p>用户在客户端询问Claude，今天XXX的天气怎么样。请求会发送到Claude的服务器，Claude判断需要调用工具来获取天气的情况，此时它会发起Function Calling，这个Calling会原路返回到我们本地的客户端。</p><p>我们的客户端上会显示一条提示信息：Claude请求调用天气查询工具，是否同意？</p><p>我们点击同意，接着我们本地的Claude客户端就会发送一个MCP调用给MCP服务器，获取到最新的天气信息，然后把这个天气信息作为一个新的请求发送给Claude。</p><p>这时候，Claude才获取到最新的天气信息，此时它会组织语言重新把结果返回给客户端。</p><p>最后我们的客户端上才会显示我们最开始问题的回复：今天XXX的天气是晴天。</p><p>显然，MCP并没有取代Function Calling。</p><h2 id="MCP的价值在哪">MCP的价值在哪</h2><p>为什么我说MCP是AI应用开发绕不过的一个协议呢。</p><p>还是以我之前写过的一个玩具项目举例，我给Qwen写过一个Function Calling。</p><p>当时需要我做什么呢？</p><p>首先，我得编写一个功能，然后作为一个开放的接口在公网暴露出来。</p><p>然后我要把这个接口通过Qwen提供的工具注册方式，把这个接口注册到Qwen里。</p><p>最后还要进行功能的调试。</p><p>整个流程下来非常繁杂，因为我作为一个开发AI应用的人，我承担了太多的责任。</p><p>比如我得负责把工具包装成接口，还得专门写一个提示词来描述这个工具。</p><p>比如我得进行功能的调试，模型的调试极其麻烦。</p><p>同时，我写好的工具，大概率以后想复用时，还得写一套代码来实现把工具传递给AI。</p><p>这还不算其中认证之类的事情，实在是过于麻烦了。</p><p>但有了MCP之后呢？</p><p>对应的功能我可以包装为MCP的Server，然后用MCP Client的SDK快速实现功能调用和传输给AI。</p><p>以后我想复用功能，我只需要继续用MCP Client的SDK写一下调用就行了，不需要再去动MCP Server的代码。</p><p>简单来说，MCP的价值就在于提供了一个良好的职责划分。</p><p>MCP Server负责提供具体的工具，同时也是Server来负责提供工具描述之类的信息。</p><p>MCP Client负责转发AI的工具调用等，同时也可以使用MCP协议提供的Sampling等能力实现一些客户端的其他功能。</p><p>LLM模型的提供商负责提供Function Calling即可。</p><p>你看，负责开发具体功能的，也负责提供了给模型的接入文档；负责开发客户端的，只需要关心怎么写胶水代码来拼凑功能。</p><p>这样就解决了AI应用最大的痛点，开发成本高昂。</p><p>有相当多的软件，也可以发布自己的MCP Server来实现无缝变身AI应用了。</p><p>试想一下，假设美团提供了MCP服务，那么是不是真的就可以做到，我们在手机的语音助手里说一句：我要点杯咖啡，然后语音助手自动帮我们利用MCP去查询美团上的信息，然后下单支付呢？</p><p>这时候回头看看官方说的那句，MCP就是type-c接口，是不是有点味道了呢？</p>]]></content>
    
    
    <summary type="html">MCP肉眼可见地火了，这次聊聊MCP。</summary>
    
    
    
    <category term="学习" scheme="https://tulancn.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="技术协议" scheme="https://tulancn.github.io/tags/%E6%8A%80%E6%9C%AF%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>canvas-editor使用记录</title>
    <link href="https://tulancn.github.io/2025/03/21/work/canvas-editor%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <id>https://tulancn.github.io/2025/03/21/work/canvas-editor%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</id>
    <published>2025-03-21T13:06:31.000Z</published>
    <updated>2025-04-07T09:08:07.034Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>主页：<a href="https://hufe.club/canvas-editor-docs/">https://hufe.club/canvas-editor-docs/</a></p><p>canvas-editor是个基于canvas/svg的富文本编辑器。</p><p>使用下来，比较优秀的点有：</p><p>1、页面类似word，并且开箱即用。</p><p>2、提供了非常多扩展的接口。</p><p>3、数据保存非常方便。</p><p>当前，也有些缺点：</p><p>1、文档不够详细，很多接口仅仅是说明了有这个接口，具体的用法都没讲。</p><p>2、事件回调的机制说明不够清晰。也算是个文档的问题。</p><p>3、没有Vue或Reactor的开箱即用包。</p><h2 id="空的回调事件">空的回调事件</h2><p>在实现工具栏时，发现了一个很抽象的事情。</p><p>官方的工具栏中，假设我选中一些文字然后加粗，工具栏会正常把加粗的按钮置为已经点击的状态。</p><p>而在我实现的工具栏中，每次点击工具栏的按钮后，会出现一个空的回调事件，把工具栏的状态置回没有点击的状态。</p><p>最后询问了官方，也翻了下对方的代码。</p><p>发现是因为官方的工具栏用的事件不是click，而是鼠标按下的事件。</p><p>我用click，所以点击的时候出现了失焦，因此有个空的回调事件出来。</p><h2 id="Word插件样式">Word插件样式</h2><p>官方的实现中，word的导入导入功能有非常严重的样式丢失。</p><p>最后是我自己写了一个导入的功能，把word解析为xml然后转化为canvas-editor的json格式。</p><p>目前导出还没实现，但大概率也得手动写一个导出了。</p><h2 id="任何操作都会被记录">任何操作都会被记录</h2><p>发现对文档的操作都会被记录。</p><p>这导致了我们用AI添加一些文字内容到文档时，明明正常执行完了，但点击回退又会回到填了一半的情况。</p><p>比如我们润色时会高亮原文，那么润色完了点击回退，就会回到原文被高亮的状态。</p><p>最后发现可以在操作时加一个参数来让本次操作不被记录。</p>]]></content>
    
    
    <summary type="html">canvas-editor是个很优秀的国产开源富文本编辑器，这里记录下我的使用。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="前端" scheme="https://tulancn.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>路由权限问题导致的一次离奇Bug</title>
    <link href="https://tulancn.github.io/2025/03/21/work/%E8%B7%AF%E7%94%B1%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%80%E6%AC%A1%E7%A6%BB%E5%A5%87Bug/"/>
    <id>https://tulancn.github.io/2025/03/21/work/%E8%B7%AF%E7%94%B1%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%80%E6%AC%A1%E7%A6%BB%E5%A5%87Bug/</id>
    <published>2025-03-21T11:13:04.000Z</published>
    <updated>2025-03-21T12:03:57.566Z</updated>
    
    <content type="html"><![CDATA[<h2 id="起因">起因</h2><p>最近重新写了一个项目，本地运行时一直没什么问题，但是部署到服务器上时出现了打开前端页面一片空白的情况。</p><img src="/2025/03/21/work/%E8%B7%AF%E7%94%B1%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%E5%AF%BC%E8%87%B4%E7%9A%84%E4%B8%80%E6%AC%A1%E7%A6%BB%E5%A5%87Bug/image-20250321191917050.png" class="" title="image-20250321191917050"><p>大概表现就是如图中的情况，打开控制台看到有三个请求，分别是获取index.html，获取js和css，但是响应中是一片空白。</p><h2 id="环境">环境</h2><p>部署是在校内的服务器，外网无法访问。</p><p>前后端都部署在同一个服务器上，都使用docker构建镜像后部署。</p><p>前端是在docker中进行打包，并且装了nginx来渲染页面。</p><h2 id="排查">排查</h2><h3 id="查看nginx配置">查看nginx配置</h3><p>遇到这个情况，我第一反应是路由转发有问题，导致前端一直在请求什么东西卡死了。</p><p>所以去翻了下nginx的配置，确实发现路由转发还写的是老的服务器。</p><p>笑死，太简单了。</p><p>修改config文件，git commit，git push，然后重新部署镜像。</p><p>结果还是一模一样。</p><p>看来并不是转发的问题。</p><h3 id="查看nginx日志">查看nginx日志</h3><p>没辙，只能去翻下docker日志。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs [container]</span><br></pre></td></tr></table></figure><p>由于做了日志映射，nginx的日志会打印在docker的日志里。</p><p>翻了下有一条报错，是没获取到logo文件。</p><p>笑死，被我找到了。</p><p>于是改了下logo的读取方式，保证能读到logo文件。</p><p>git commit，git push，然后重新部署镜像。</p><p>打开页面，发现还是不行。</p><p>不信邪，再翻了下日志，已经没这个报错了。</p><p>看来也不是logo文件读不到的问题。</p><h3 id="查看docker脚本">查看docker脚本</h3><p>再去看了下docker的脚本，可能是端口映射的问题。</p><p>查看后发现也没什么问题。</p><p>但是想了想，也许是网络问题，修改了启动命令，加上了<code>--network=host</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm --network=host your_image curl http://localhost:8000</span><br></pre></td></tr></table></figure><p>这样docker会直接使用宿主机的网络配置，也就是没有端口映射这一说了。</p><p>某些情况下这样性能会好一些。</p><p>但改了之后还是没解决。</p><h3 id="查看docker内部文件">查看docker内部文件</h3><p>在看看nginx的运行日志吧。</p><p>先进入docker的镜像内部，这里用shell，因为镜像没装bash，不然用bash更方便些。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it [container] sh</span><br></pre></td></tr></table></figure><p>找了下nginx的日志，发现没什么异常。</p><p>再找了下前端的静态文件，也没看出来什么问题。</p><p>不信邪，试试在docker内ping了一下后端，发现也能ping通。</p><p>那看来网络是没什么问题了。</p><h3 id="查看本地打包的文件">查看本地打包的文件</h3><p>本地打包后的文件，我试着用serve来运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx serve -s dist</span><br></pre></td></tr></table></figure><p>运行后打开localhost，发现页面是正常的。</p><p>看来本地文件没问题。</p><h3 id="在服务器上打包">在服务器上打包</h3><p>重新登录服务器，在服务器上用npx serve来运行后端的文件。</p><p>提示没有npx。</p><p>行吧，那先安装npm。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install npm</span><br></pre></td></tr></table></figure><p>再执行，说node版本太低，只有12。</p><p>那再升级node。先装个nvm。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash</span><br></pre></td></tr></table></figure><p>然后让nvm生效。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.nvm/nvm.sh</span><br></pre></td></tr></table></figure><p>最后nvm安装node 18，再切换到18。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvm install 18</span><br><span class="line">nvm use 18</span><br></pre></td></tr></table></figure><p>用node的命令查了下，确实是18了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure><p>终于完事，再用npx serve启动。</p><p>发现打开是404。</p><p>？？？</p><p>看了下是服务器上没build后的静态文件。</p><p>行，那先安装。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br><span class="line">npm run build</span><br></pre></td></tr></table></figure><p>在本地用浏览器打开，居然也是一片空白。</p><p>？？？</p><p>至少复现了是吧。</p><p>看来不是网络问题，就是前端打包后的静态文件有问题。</p><p>脑袋里面灵光一闪，重新在本地起了下前端，这次不用localhost，用本机的IP来访问。</p><p>发现也是空白。</p><p>这下在本地复现了。</p><h3 id="询问AI">询问AI</h3><p>成功复现至少是成功了90%。</p><p>目前看下来，关键点在与用localhost能正常访问，而使用其他的IP则不行。</p><p>问了下AI，可能是什么原因。</p><p>提示说可能是cros，也可能是使用的命令不对，还有可能是防火墙问题。</p><p>都试了下，还是没解决问题。</p><h3 id="回退版本">回退版本</h3><p>最后无奈之下，开始回退版本。</p><p>用git命令回退到之前的版本，看看是哪个提交导致了目前的问题。</p><p>首先回退到之前改了logo的那次提交，发现不行。</p><p>再回退到这个提交的前一个。</p><p>启动，居然正常显示了？</p><p>ok，锁定是这个提交的改动有问题。</p><p>于是逐个逐个回退文件的改动。</p><p>回退到修改主页重定向的时候，页面正常显示了。</p><p>好，找到问题了。</p><h2 id="修复">修复</h2><p>分析一下，是路由守卫和主页重定向有冲突。</p><p>当时让未登录的用户会重定向到一个文档管理页面，而这个文档管理页面被路由守卫了，需要用户进行登录才可访问。</p><p>一来一去，死循环了，于是页面就卡死了。</p><p>那么为什么用localhost可以正常访问？</p><p>因为有浏览器缓存，会保持用户的登录状态。而其他的路由没有缓存，需要重新登录，于是就会进入死循环。</p><p>修改了这个逻辑，添加了一个未登录也可访问的落地页，让用户先在这个页面登录。</p><p>修改后重新构建，已经能正常渲染了。</p><h2 id="结论">结论</h2><p>重新看下来，debug的过程就是要对整个部署流程、前端渲染过程有一定的了解。</p><p>我还是有些依赖个人的经验，导致我直接从最大可能的地方入手，进行排查。</p><p>当然也确实一下子就找到了一个问题（虽然和最终的问题无关）。</p><p>但仔细想想，主要也是对前端不熟悉，很多时候就缺少了些验证手段。</p><p>这次都是现査工具现用的。</p><p>不过这样解决问题，才能让人有所成长吧？</p>]]></content>
    
    
    <summary type="html">这两天遇到了一个很奇怪的Bug，最后的错误原因也令人啼笑皆非，这里记录下排查过程。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="前端" scheme="https://tulancn.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>SSE协议和流式输出</title>
    <link href="https://tulancn.github.io/2025/03/09/work/SSE%E5%8D%8F%E8%AE%AE%E5%92%8C%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/"/>
    <id>https://tulancn.github.io/2025/03/09/work/SSE%E5%8D%8F%E8%AE%AE%E5%92%8C%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA/</id>
    <published>2025-03-09T03:17:03.000Z</published>
    <updated>2025-03-21T11:16:40.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="流式输出">流式输出</h2><p>大多数的聊天机器人都会用到流式输出。为什么一定要用这能力呢？</p><p>任何的LLM的回答都是逐字逐句生成的，因此使用同步调用一次获取所有的消息，会造成长时间的阻塞。</p><p>尤其是DeepSeek R1这类推理模型，在给出正式的回答前还有大段大段的思考文本，这让用户的等待时间更加长了。</p><p>同时，一个请求的超时时间过长会带来其他副作用，比如用同步调用很难判断是LLM服务挂掉了还是单纯生成内容很长所以一直没回答。</p><p>因此，为了提升用户体验，减少后端一些不必要的麻烦，流式输出在目前的AI应用中越来越重要了。</p><h2 id="SSE-Server-Sent-Events-协议">SSE (Server-Sent Events) 协议</h2><h3 id="SSE协议概念">SSE协议概念</h3><ul><li>SSE（Server-Sent Events）是一种服务器推送技术，允许服务器向客户端发送事件流</li><li>它建立在HTTP协议上，使用标准的HTTP连接，但允许服务器持续向客户端推送数据</li><li>SSE连接是单向的（只能服务器向客户端发送数据），与WebSocket不同（WebSocket是双向通信）</li><li>SSE适用于实时通知、实时日志、聊天应用等场景</li></ul><h3 id="SSE协议技术特点">SSE协议技术特点</h3><ul><li>使用标准HTTP连接，不需要特殊协议或端口</li><li>自动重连功能，断开连接后会自动尝试重新连接</li><li>使用纯文本传输，每条消息格式为<code>data: 消息内容\n\n</code></li><li>支持事件ID和事件类型，便于客户端处理不同类型的消息</li><li>比WebSocket更简单，更容易实现，但功能稍弱</li></ul><h3 id="技术选型">技术选型</h3><p>后端使用的是<code>FastAPI</code>，本身就有很好的流式输出支持。</p><p>前端则是使用了<code>@microsoft/fetch-event-source</code>库，因为这个库允许修改请求头的内容，方便做鉴权。</p><h3 id="项目中SSE的应用">项目中SSE的应用</h3><ol><li><strong>后端实现（FastAPI）</strong><ul><li>使用<code>FastAPI</code>的<code>StreamingResponse</code>实现流式响应</li><li>通过异步生成器<code>serialize_generator</code>生成SSE数据流</li><li>标准SSE格式: <code>data: JSON数据\n\n</code></li><li>使用<code>time.sleep(0.05)</code>控制响应速率，避免客户端接收过快导致丢包</li></ul></li><li><strong>前端实现（Vue 3 + TypeScript）</strong><ul><li>使用<code>@microsoft/fetch-event-source</code>库处理SSE连接</li><li>在<code>frontend/src/utils/request.ts</code>中封装了<code>sseRequest</code>函数</li><li>支持请求头设置、身份验证、错误处理和连接状态管理</li><li>通过<code>parseSSEMessage</code>函数解析接收到的SSE消息</li></ul></li></ol><h2 id="打字机">打字机</h2><h3 id="为什么要做打字机效果">为什么要做打字机效果</h3><p>由于网络波动，后端的推送并不是匀速到达客户端的。如果前端仅仅是收到一条消息就拼接到前端的文本上，那么最终效果就会显得很呆。</p><p>通过在前端添加一个缓冲队列，来让字符匀速显示，会让前端的展示效果显著提升。</p><p>就是俗话说的&quot;质感&quot;。</p><h3 id="核心功能">核心功能</h3><ul><li>实现了文字逐字打印的动画效果</li><li>支持动态调整打字速度</li></ul><h3 id="技术实现">技术实现</h3><ul><li>使用TypeScript实现类封装</li><li>采用缓冲区设计模式存储待显示的文本</li><li>使用定时器控制打字速度</li><li>提供丰富的回调函数支持，如<code>onComplete</code>、<code>onPause</code>等</li></ul><h2 id="总结">总结</h2><p>整体逻辑其实很简单，SSE+前端打字机。</p><p>前端本身就是响应式的，所以代码实现其实不麻烦，直接往文本框里拼接字符就行，甚至有点简单。</p><p>但不管怎么说，SSE协议在AI应用中目前使用应该是最为广泛的，几乎所有项目都得走这个流程。</p>]]></content>
    
    
    <summary type="html">最近在项目中用到了SSE协议，这里简单写下怎么接入的。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="技术协议" scheme="https://tulancn.github.io/tags/%E6%8A%80%E6%9C%AF%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>我也是全栈</title>
    <link href="https://tulancn.github.io/2025/02/28/work/%E6%88%91%E4%B9%9F%E6%98%AF%E5%85%A8%E6%A0%88/"/>
    <id>https://tulancn.github.io/2025/02/28/work/%E6%88%91%E4%B9%9F%E6%98%AF%E5%85%A8%E6%A0%88/</id>
    <published>2025-02-28T14:07:08.000Z</published>
    <updated>2025-03-10T13:49:45.421Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经历">经历</h2><p>如果说我是什么时候有这个想法，那我觉得最早应该要追溯到大一的时候了。</p><p>当时刚入学，正在军训。我们有一次思想测验，当时年级里有同学搭了一个网站供我们考试。</p><p>刚上大学的我对这操作惊为天人。</p><p>后来从别人那了解到，这些东西在大二的时候基本都会教。</p><p>可惜，事实是直到大三我都不知道他们是怎么搭建的这网站。</p><p>接着就是实习，才开始对软件开发有了些概念。了解了前端、后端，各种开发语言，各种基础概念。也算是开始学以致用了。</p><p>毕业之后，我开始工作。</p><p>还记得上班的第一天，部门最大的领导过来有个谈话的环节。</p><p>海航总问我们，你们有什么目标吗？</p><p>没人回应。</p><p>稍稍沉默之后，我鼓起勇气回答道：“我想做全栈。”</p><p>我觉得这就是我开始有做全栈的决心的时候。</p><p>时光飞逝，我不断加深着后端的技能。数据库，运维部署，监控，开发框架，缓存，流量治理，网络通讯，这些点在我工作的这些年里我也慢慢加深了解。</p><p>唯独前端一直没什么进展。</p><p>我一直就是维持在有个前端项目，可以在我本地运行起来的这么个程度。</p><p>第一次转机出现在第二年年末，我去做低码平台的项目。</p><p>当时前端资源紧缺，很多细节让前端来改实在是有些慢了。我就看了下vue2的语法，还有前端的基础入门，就上手改一下样式和bug之类的。</p><p>当时算是有了个基本的了解，大概前端是怎么个运行逻辑是知道了。</p><p>第二次转机出现在上研究生之后，我参与了一个学校的项目，主攻前端。得益于大模型的发展，学习速度非常快。</p><p>再加上有cursor之类的编程工具协助，我很快就能独立开发出非常美观的前端界面了。</p><p>当然我也是看了些课程，以便了解前端的基础语法。这里主要是ts和vue3不懂，看了下尚硅谷的b站课程。效率不是很高，但我觉着也算是懂了不少吧。</p><p>上学之后也有些其他的经历，比如用python做数分的项目，尝试写了些微信小游戏和小程序。</p><p>就我个人而言，我觉得自己确实也算是真正成为一个合格的全栈开发了。</p><p>能独立完成前端、后端、移动端、数据库和运维部署的所有工作，作为打工人已经没什么太多可求的了。</p><p>我有段时间的QQ签名改成了一人成军，意思就是形容全栈可以一个人完成所有的开发工作。</p><p>写代码确实是很有意思的事情。</p><p>接触的东西多了以后，我开始有个想法：软件开发的各类工具只是表象，底层有两个东西，一是技术思维，二是业务知识。</p><p>技术思维有两种，一是直觉性的，看到一个东西就能反应过来应该怎么做，二是思考后得出的。</p><p>变为全栈之后，对我的技术直觉有了显著的提升。大多数的功能或架构，都能说上几句话，或者是在短短的时间内了解和掌握了。</p><p>我常常过于信任直觉性的思维，二忽视思考的结论，这是不对的。</p><p>业务知识是对具体业务场景的了解。技术总是趋向于无限增长的，总想得到最优解。但业务不是，业务知识充斥着边界和特例。这些边界和特例会反过来限制技术，让技术在到达某个界限后，就趋向于停止。</p><p>只有业务发展了，才会对技术有需求。否则技术发展更多的是一种学习研究性质的工作，不是用来解决具体问题的，没有落地的地方。</p><p>技术应该有前瞻性，但不应该过度，否则会对其他的资源造成挤压。</p><h2 id="前后的对比">前后的对比</h2><p>然后再聊聊我学了前端之后的理解。</p><p>前端我理解中更像是用代码在布设各种陷阱，设置好了之后等用户点击触发，再改变页面的逻辑。前端的数据来源之一是用户，用户在某个组件填写的内容，得直到点击发送按钮时才会用上。</p><p>后端则是从收到请求之后，运行各种复杂逻辑进行计算。</p><p>区别在于后端不需要布设多个组件进行组件的联动，后端收到的用户请求中就是所有的信息，其他信息得从数据库或其他持久化数据源用获取。后端更多的是数据的流转和计算。</p><p>这其实解释了为什么前端总是响应式的，前端的回调逻辑远多于后端。</p><h2 id="未来发展">未来发展</h2><p>现在讲究大前端，会写web，适当学一下就能写移动端和桌面端。</p><p>挺好笑的就是前端现在有点TS一统天下的味道了，而TS的语法总觉得和Java或者Python一个味道。</p><p>或者说未来大家的编程语言都会变成越来越像吧。</p><p>就像Java缺少了很多语法糖，而Python则充斥着各种语法糖，Go学了Python，也搞了很多类似的语法。</p><p>现在Java 21出来了，也在往这方向靠。</p><p>新时代的编程语言似乎在趋向于统一，都在往高级编程语言的方向进化。这也是这么多年来，无数程序员编码过程中的实践而产生的经验了。</p><p>编程语言这东西没必要卡这么死，大家到最后其实都差不多。</p><p>搞全栈不太好的地方就是样样都会，但很难样样都精通。以后可能最好还是走中间人或者管理的方向会比较好，毕竟啥都懂的人也是少数，要取长补短嘛。</p>]]></content>
    
    
    <summary type="html">最近突然发现自己已经可以算是全栈了，想回顾一下自己的心路历程。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="工作总结" scheme="https://tulancn.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>聊聊AI工具和学习</title>
    <link href="https://tulancn.github.io/2025/02/15/study/%E8%81%8A%E8%81%8AAI%E5%B7%A5%E5%85%B7%E5%92%8C%E5%AD%A6%E4%B9%A0/"/>
    <id>https://tulancn.github.io/2025/02/15/study/%E8%81%8A%E8%81%8AAI%E5%B7%A5%E5%85%B7%E5%92%8C%E5%AD%A6%E4%B9%A0/</id>
    <published>2025-02-15T11:54:17.000Z</published>
    <updated>2025-03-10T13:54:25.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>上次nlp的project被同学宣传了一下，后来有人来交流，就被拉去给老师做项目了。</p><p>名义上是当了student assistant，工资也是按照这个待遇来。80港币一小时，说实话还不少。</p><p>项目是关于一个AI论文的平台，希望能借用AI工具来对论文进行编写、修改和润色。功能倒是挺简单的，我觉得主要难点还是在设计和实现上。</p><h2 id="难点">难点</h2><p>我觉得第一个难点是人力的缺失，更准确地说是协作的方式。虽说我后来又找了3个人来，组成了6人小队，但还是觉得人手不够，做事不够快。</p><p>从数量上来说，这点人肯定是够了，但我觉得暂时没找到很好的协作方式。换句话说，也就是没能把这6个人的能力全部发挥出来。</p><p>这也是很痛苦，没什么办法的事情，毕竟项目管理本就是一个很困难的事情。</p><p>第二个难点应该是设计的问题。目前没有产品经理的角色，实际上很多功能的设计上是缺失了有人把关。甚至更严重的是，我们所有人都是身兼多职，既要看产品怎么样，又要看怎么实现。</p><p>这种体验倒是和初创团队差不多了。</p><p>没有专业的设计团队，难免会导致功能上可能少东少西，甚至交互上也会很反人类。</p><p>第三个难点在于测试和验收。目前也没有测试人员，功能的验收是没有人把关的。做得怎么样，怎样算好，有没有bug，这些大家在做完之后都没有反馈的渠道。</p><p>没有结果的反馈，也就很难进一步提高了。</p><p>第四个难点在于缺少标准的制定。管理团队最重要的就是制定标准，标准是协作的基础。人并不可靠，但完善的标准可以让人变可靠。这话对AI也同理。目前我们后台日志打印的方式、异常的抛出、前端的请求方式都没有统一的标准制定出来。这些得尽快解决了。</p><h2 id="预期">预期</h2><p>聊完难点，不妨畅想一下预期。</p><p>我理想中的团队，应该是抱有热情、持续产出、愿意钻研的。他们每天都能提交一定的代码，推进一定的工作。定时上线开会，有问题及时沟通。大胆指出目前工作中不合逻辑的地方，并且提出改进意见。</p><p>显然目前还达不到。</p><h2 id="使用AI工具开发">使用AI工具开发</h2><p>前面感觉都扯远了，这里回到正题，再聊聊AI工具和学习。</p><p>这里的AI工具，应该说特指cursor这类编程工具了。</p><p>比较好笑的是，我居然是这次项目中负责写前端的人。可实际上我并不懂前端，ts基础语法都没认真学过。</p><p>但目前看下来，我甚至都不算是拖后腿的那个人。</p><p>感谢cursor，让我有了这种体验。</p><p>前端我也不是没学，也花了点时间看了下尚硅谷的Vue3前端入门教程。只不过暂时还没看完罢了。</p><p>我觉得过段时间可以把尚硅谷的项目完整做一下，还是挺有意义的。</p><p>整体编写项目前端的过程，让我觉得最重要的还是知道目标，然后提出自己的需求。</p><p>AI工具已经非常智能，只要在输入指令的时候带上那么一点点技术的关键词，基本上就可以帮你实现各种需求了。</p><h2 id="使用AI工具学习">使用AI工具学习</h2><p>但凡事皆有代价。使用AI工具来开发，其实很大程度上属于是用长期的提高和理解，换取暂时的成果。</p><p>使用cursor我写了相当多的代码，但代价就是我还是觉得自己对Vue3一窍不懂，不能写代码。</p><p>感觉是离了cursor不会写代码了。</p><p>这种体验很差，短期看到有美观的页面生成确实让人多巴胺快速分泌，有满足感。</p><p>这种感觉一旦过去，充斥心头的是一种迷茫和空虚。</p><p>频繁使用AI只会让自己变得没有长进。你增长的只有使用AI的能力，而不是做事情的能力。</p><p>这也让我觉得有些恐慌。</p><p>也是第一次有了AI是不是会取代我的工作的焦虑感。</p><p>最讽刺好笑的事情也是在这。如果现在想要快速学习新的知识，借助AI工具也是最快的。</p><p><strong>如果你愿意在每次生成代码之后，让AI工具解释一下它为什么这么写，它会是最好的导师。</strong></p><p>我觉得现在学一个新技术，最难的还是不知道roadmap。</p><p>而AI打破了这个知识搜集的过程，大部分时候它是可以提供一些可靠的roadmap，让你知道自己应该先学什么，后学什么。</p><p>我个人比较喜欢的路径是，先quick start看到成果，然后逐步深入其中的每一个步骤，理解其原理。</p><p>凭借多年的技术直觉，我觉得很多时候自己还是可以问出一些很有用的问题，从而获得最合理的AI答复。</p><p>动手学习确实比单纯听讲要好得多。</p><h2 id="总结">总结</h2><p>最近是接了一个项目，遇到了不少困难。</p><p>然后我也开始写前端了，在编写的过层中我大量使用了AI。</p><p>这让我思考为什么要用AI，AI的能力边界在哪。</p><p>我决定以后要慎重使用AI，在使用完之后多问一句为什么，从而实现在实践的过程中提高自己的技术水平。</p>]]></content>
    
    
    <summary type="html">最近在跟着老师做项目，尝试梳理下自己的一些想法。</summary>
    
    
    
    <category term="学习" scheme="https://tulancn.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习总结" scheme="https://tulancn.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>股市投机的原则</title>
    <link href="https://tulancn.github.io/2024/12/13/life/%E8%82%A1%E5%B8%82%E6%8A%95%E6%9C%BA%E7%9A%84%E5%8E%9F%E5%88%99/"/>
    <id>https://tulancn.github.io/2024/12/13/life/%E8%82%A1%E5%B8%82%E6%8A%95%E6%9C%BA%E7%9A%84%E5%8E%9F%E5%88%99/</id>
    <published>2024-12-13T08:49:54.000Z</published>
    <updated>2025-03-10T13:55:47.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="A股的底层逻辑是博弈">A股的底层逻辑是博弈</h2><p>这是对A股市场中股价波动的解释。在A股市场中，几乎没有人是通过分红的复利来实现财富增长和价值投资的。</p><p>那么，所谓的政策利好其实也只是给赌徒们指明了一个方向：下一波我们炒这个。</p><p>各类大资金做局，通过政策指引等方式制造热点，让散户和其他游资集中到某个股票中，然后大资金们出手给小资金，小资金出售给散户。</p><p>这里分两种情况来讨论：</p><p>第一种：新热点出现导致某个版块普涨。某些政策出台后，会对某些版块产生明显的利好。这时候短线资金会集中到这个版块中，不同资金会随机寻找版块中的某个股票买入，导致版块中的股票呈现出不同的涨幅。</p><p>某些股票涨幅大，那么就会吸引更多的散户和其他大资金来买入，最终表现就是封板。</p><p>一个版块中可能有多支股票都是涨停的，那么第二天就会继续在这些股票中寻找能继续封版的股票，也就是所谓的一进二，二进三等。</p><p>假如有其他的热点出现，可能这些昨天涨停的票今天都会跌。</p><p>假如市场上的其他人不认账，那这些票可能今天也会继续跌下去。</p><p>还有种可能是其他人继续追高，在昨天形势较好的股票中选择某几个继续大量买入，这时候就会出现连板。</p><p>连板中，会有些票会掉队，因为市场上参与的资金是有限的，击鼓传花的游戏在某一个时刻会因为没有其他人来买而结束。</p><p>资金会集中向头部的股票，直到头部的股票价格过高，没有人愿意再买了。</p><p>如果热点足够吸引人，还会有补涨。就是头部的连板票带动了市场中其他版块的活跃资金也参与到本版块中，其他的活跃资金可能因为前排的连板票买不到而选择去买入同版块还没炒起来的股票。这种被其他资金选择的票可能也会连板，并且在头部的连板票破板时，成为市场上资金新的宠儿。</p><p>头部的连板票，一般称为龙头；补涨的票，称为补涨龙。还有些票，一直随着版块在偷偷涨，但没有涨停，这种称为中军，一般是市值较大的白马股。</p><p>第二种：版块的资金回流。龙头崩了之后，势必会带动大量的资金一起抛售，这时候股价跌停都是正常的。</p><p>但是在跌停时，可能有资金认为这票还能涨，于是在低位进行接盘。当有足够多的资金认为还能涨时，就会出现弱转强，也就是资金的回流。</p><p>比如昨天龙头跌停，预期是今天继续跌停。但是实际上开盘是正，并且股价起飞，那么就可以认为有人觉得这票还能涨。如果弱转强的共识足够，那么就能把本来短板的龙头重新抬起来继续涨。</p><p>弱转强是非常常见的，同时也是相当有效的赚钱手段。</p><p>这里讲了两种博弈的方式，其实简单概括，就是先信卖给后信。</p><h2 id="设立止损目标">设立止损目标</h2><p>在赚钱时，也应该设立止损目标，比如利润回撤到3%就清仓。</p><p>无论如何，少赚总比不赚好。如果利润回撤了，就说明肯定有更好的清仓时机，应该去思考什么时候清仓更合适，而不是死扛把利润都亏空了。</p><p>在亏钱时，也要设立止损目标。止损是个麻烦事儿，大部分人总会妄想自己后面可以重新赚回来。但止损时应该想的是自己为什么当时会去买入这个股票，说明需要优化买入时机。</p><h2 id="不做自己不了解的行业">不做自己不了解的行业</h2><p>自己不了解的行业，可能会导致决策失误，进而导致亏钱。</p><p>只做自己了解的行业是对自己负责。</p><h2 id="不做太小的票">不做太小的票</h2><p>如果一个票的市值过小，可能随意一个游资都能在这票坐庄。</p><p>庄股是最恶心的，上涨和下跌没有任何规律可言。</p><h2 id="尽量做股性活跃的票">尽量做股性活跃的票</h2><p>股性活跃，指的是这个版块有利好时，大部分资金都会首先选择买入这个股票。这种股票在长期活跃的版块都有存在。</p><p>可以说大家可能对老龙都有记忆吧，每次有利好都优先把以前连板过的票拉出来再炒一次。</p><h2 id="少碰kdj在20-80之间的票">少碰kdj在20-80之间的票</h2><p>kdj指标说明了短期一支股票的超买和超卖的情况。如果一个股票的kdj在20-80之间，大概率说明这票最近没什么人在炒。</p><p>没人炒的票持有了也很难获得大量收益。</p><h2 id="遵循自己的规则">遵循自己的规则</h2><p>设立自己的交易规则，严格遵守规则，并在复盘时适时调整规则。</p><p>交易时间一定要按照自己的交易规则来进行交易，东买一个西买一个总有一天赚的都要亏回去。</p><p>如果一次交易是亏钱的，说明交易规则有问题，想办法调整并避免以后还出现类似的问题。这种态度才是正确的。</p>]]></content>
    
    
    <summary type="html">股海浮沉，总结了几条对自己有帮助的原则，希望自己能继续遵守。</summary>
    
    
    
    <category term="生活" scheme="https://tulancn.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="投资" scheme="https://tulancn.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>NLP的project复盘</title>
    <link href="https://tulancn.github.io/2024/11/22/study/NLP%E7%9A%84project%E5%A4%8D%E7%9B%98/"/>
    <id>https://tulancn.github.io/2024/11/22/study/NLP%E7%9A%84project%E5%A4%8D%E7%9B%98/</id>
    <published>2024-11-22T06:57:31.000Z</published>
    <updated>2025-03-10T13:56:09.342Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>这是第一门需要协作的大作业，过程中着实暴露了很多问题。我觉得是需要复盘来明确这些问题，以免在下次的作业中再有类似的情况发生。</p><p>项目要求是利用大模型的技术，结合课上所学的NLP相关知识，来开发一个对话模型。最终这个系统能做到让用户通过自然语言来查询某个领域的数据。</p><h2 id="过程">过程</h2><h3 id="第一次会议">第一次会议</h3><p>一开始可能是大家还不熟，我作为脸皮最厚的主动出来破冰。</p><p>这就像无领导小组讨论第一件事就是选领导；分布式等价节点一致性算法第一件事就是选主节点；Project第一个跳出来的人也会成为组织的核心。</p><p>之后就基本围绕着我的思路，开始选题，分配预研任务。我确定了一下开会的频率和任务的大致分配方向，以及日常的沟通方式，代码提交的仓库，保证这个项目推进必要的一些因素都已具备，第一次会议就算结束。</p><p>打算是分三周，第一周预研，第二周产出第一个demo，第三周产出最终的。</p><h3 id="第二次会议之前">第二次会议之前</h3><p>我这边主要是要预研RAG的基础框架和前端界面。</p><p>说实在的我也没多上心，粗看了一下AnythingLLM，跑了下本地模型发现展示的效果还可以。然后接入本地模型、通义千问和其他的一些模型试了下，认为这个框架基本可用，就算完事了。</p><p>数据爬取的任务倒是做得还可以，能拉到三个网站的数据。</p><p>需求分析决定了后续的提示词怎么写，这部分最终产出不是很标准，但也算有模有样，可以列入工作内容的一部分。</p><h3 id="第二次会议">第二次会议</h3><p>原先预订是第二次会议前要有产出的demo，所以第二次的会议上我就展示了一下AnythingLLM。其他的一些沟通已经没多少印象了，大概就是确定要做什么之后，直接进入模型微调的阶段了。</p><h3 id="第三次会议">第三次会议</h3><p>原先预定是在第二周产出一个demo，但是用AnythingLLM这个demo产出比我想得要快太多了…</p><p>然后第二周的目标在周二就算提前完成，摸了会儿鱼一直拖到第三周才进行第三次会议。</p><p>这第三次会议上演示了一下纯使用AnythingLLM的成果，客观来说包装一下作为汇报用的结果还真可以。</p><p>不过还是想着再做些事情，于是会后我又花了点时间去写了下插件。</p><h3 id="第三次会议后">第三次会议后</h3><p>由于我也没怎么写过python的工程，先花了点时间看了下python的工程结构。</p><p>插件开发需要模型支持，就想着先把模型从AnythingLLM搬到了阿里云的百炼平台上，花了一个小时吧。</p><p>看了下模型怎么自定义插件的文档，本来还想找找阿里云有没有提供案例，结果发现没有。</p><p>那没辙，先理解再说。</p><p>翻了下就是自己部署一个web服务，然后提供OpenAPI 3.0的接口文档和一个告知模型在什么情况下需要调用插件的prompt。那就简单了，实际的功能代码有现成的，我只需要把这些能力集成到web服务里就行。</p><p>于是开始翻怎么用python写web服务，发现有个quart的框架，快速写了demo，结论是可行。</p><p>找了下阿里云有免费的试用服务器，租了一个4核8G的服务器，ping了它的公网IP，通了。那这服务器也好说。</p><p>部署服务到云服务器，一调用发现不通。怀疑是权限问题，去控制台找了下权限控制的功能，开通端口，再调用通了。这部署也算完成。</p><p>用AI生成了OpenAPI 3.0的接口文档，接着写prompt，最后在百炼上创建一个插件，这样就算集成了。</p><p>调试了一下大模型调用插件的效果，发现还不错，至此完成了插件的功能。</p><p>最终的效果给同组的同学展示了一下，都说很可以。给他们开通了子账号的权限，每个人都能调整模型的prompt和看到最终的效果。</p><p>随后的时间就是准备pre和最终的report。</p><h2 id="复盘">复盘</h2><h3 id="问题分析暴露了思维的差异">问题分析暴露了思维的差异</h3><p>简单分析一下，在模型侧，要通过一些手段来让模型了解某个领域的数据。那么很自然就会想到专家模型，想到微调，接着答案就呼之欲出——RAG。</p><p>对，其实能做到RAG，这个项目基本已经完成了。</p><p>那么就涉及到数据来源的问题，这里就出现了第一次的思维差异。我倾向于额外添加插件的能力，能让模型获取到更具有时效性的信息。而其他人似乎并没有意识到这个能力的必要性，认为这个能力优先级不高，能先处理好RAG就可以了。</p><p>其次就是这个选题的方向是什么，大家一开始都没什么点子，于是我提出用头脑风暴的方式来发散思维。最终确定是做一个关于招聘信息的助手。</p><p>到最后，我干脆直接拉出日程表，列了下整个工作的日程和计划，也拟定了一些里程碑目标出来。大家也没多说什么，一下子就同意了。</p><h3 id="工作方式的妥协">工作方式的妥协</h3><p>如果是企业中接到一个项目，可能我第一反应是确定好日程计划，然后拉人开会，对齐拉通保证大家思维的一致性，留好联系方式，有什么问题及时调整。</p><p>但是实际在这个project的执行过程中，一来大家都挺陌生，不知道对方几斤几两；二来工具不齐，想做项目管理多多少少有些麻烦。最后只能妥协，用腾讯文档作为项目管理的一环，github作为配置库，以及任务分配全靠自觉。</p><p>所幸大家都还算给面子，仗着年纪大脸皮厚大家还算愿意听我讲话。</p><p>还有就是远程办公，在微信群沟通确实效率过于低下了，我个人会更喜欢用远程会议的方式来快速沟通。而且大家也不是集中在一处专心做这个project，互相之间配合和协调也是个问题。</p><p>最终妥协的成果，就是把任务提前分隔成较为独立的子任务，每个人认领其中一部分。然后我作为最终的统筹，收集各个成员的产出，并拼凑为一个较为合理的产物。</p><p>就我个人而言，这些都不算什么工作量，但是对于其他人而言可能会有些困难，毕竟没什么管理经验。所以我来担任也算是比较合理的。</p><h3 id="留好成长的空间">留好成长的空间</h3><p>整个过程中一直在强调要沟通，每个人的提交都能让其他人在本地运行。本意是想让每个人都参与进来，能看到project的当前的效果。</p><p>但第一个迭代这个事情做得不是很好。所以第二个迭代我就全面上云，让每个人都能参与进来。</p><p>我觉得一个项目的推进过程中，项目中的每个人都应该是有所成长的。作为领导，一定会注意经验的复用和下属的培养。</p><p>对于这个project来说，就是要给愿意学的人，留好空间，把想学的东西放在每个人随时都能够得到的地方。</p>]]></content>
    
    
    <summary type="html">管理亦是一门妥协的艺术。不要以企业的标准来要求学生。</summary>
    
    
    
    <category term="学习" scheme="https://tulancn.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习总结" scheme="https://tulancn.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>回炉重造是种什么样的感觉？</title>
    <link href="https://tulancn.github.io/2024/11/20/life/%E5%9B%9E%E7%82%89%E9%87%8D%E9%80%A0%E6%98%AF%E7%A7%8D%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E6%84%9F%E8%A7%89%EF%BC%9F/"/>
    <id>https://tulancn.github.io/2024/11/20/life/%E5%9B%9E%E7%82%89%E9%87%8D%E9%80%A0%E6%98%AF%E7%A7%8D%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E6%84%9F%E8%A7%89%EF%BC%9F/</id>
    <published>2024-11-20T05:32:12.000Z</published>
    <updated>2025-03-10T13:55:54.636Z</updated>
    
    <content type="html"><![CDATA[<h2 id="就读体验">就读体验</h2><p>整体来说，在polyu的就读体验还算不错。</p><p>各种流程都会有较为详细的指引。比如入学、选课、课程的考试之类的日常流程，都有邮件会提醒，邮件中会有很详细的说明，足够每个人完成这些。</p><p>每个学生也都会收到不少附赠的资源，比如学生的云电脑有两块免费的4090ti可用。在学校想训练自己的大模型，或是做一些微调都足够用了。</p><p>同学倒是也都挺实在的。毕竟排名是港三之外吧，挺少见到本科985的同学，遇到的985也大多是转方向来读计算机的。像我这种工作4年再来读书的确实是少数的少数。这也导致了遇到的同学大多很佛系，不太在意成绩，大多是一个能过就行的态度。某种意义上倒也挺符合对港留子的刻板印象的。</p><p>虽说绝大部分人都是抱着一个拿到学位就算成功的态度，但同学中也有少数比较卷的，大部分是local，目标都是继续深造读博。</p><p>学校的课也不算太水，NLP、AI concept和Big Data都会介绍目前最前沿的技术。对于我这种没接触过AI算法的人来说，确实有点头大。不过也算学到了新东西，倒也不亏。</p><p>本科毕业之后，可能大家都已经有了自己的规划。对于读书这事，目的也各不相同。</p><p>这港硕的一年，就像是把一群各有自己想法的人聚集到了一块儿，每次遇到陌生同学，聊几句就会发现对方过着和你截然不同的人生。</p><p>从我个人角度，满分100，我给港理工打90。10分扣在不是港三，但这事也怨我，早点申请可能也就到港三去了。</p><h2 id="回炉重造的成果">回炉重造的成果</h2><p>挨过了社会的毒打，重新回到校园，以为会是狼入羊群嘎嘎乱杀，结果是发现牛混到了羊群里，居然要重新学怎么吃草。</p><p>技术栈差别很大，真的很大。我原本是搞工程方向的人，但是研究生也不得不去学算法，学AI的技术了。</p><p>大模型的风确实刮到了各种地方，现在是个学科只要挂上机器学习的名头就能变成一个新学科，一个新方向。</p><p>风来了，那么也只能顺着风的方向去飞。借着学校的课程，我也开始在AI这块入了个门。</p><p>到NLP的小组作业，也算是小小地喷发了一会儿。</p><p>我久违地写了点代码，然后把服务部署到了阿里云的云服务器上，最后接入通义千问大模型。搞出来的效果让同组的人都惊呆了。</p><p>其他组大多是在大模型的上层做了RAG，但我的思路是在RAG之外要给模型提供插件来获取实时信息，否则这个项目的成果完全不可用。</p><img src="/2024/11/20/life/%E5%9B%9E%E7%82%89%E9%87%8D%E9%80%A0%E6%98%AF%E7%A7%8D%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E6%84%9F%E8%A7%89%EF%BC%9F/image-20241121203740450.png" class="" title="image-20241121203740450"><p>做完的时候有种感觉，就是自己好像确实学了不少新玩意儿。</p><p>这次回炉重造，最初的目标就是能开拓一些新的知识点，希望能学python，学数据分析，学AI训练，学会写前端。</p><p>我觉得还是有些进步的，至少python会写了，数分也入门了，大模型的训练、微调也会了。目前还差点前端，打算是考试期间学点微信小程序的前端，先入门一下。</p><p>也慢慢明白自己擅长和不擅长的地方，对未来的规划也更加明确了。</p><p>回炉重造初显成效。</p><h2 id="看过猪跑很重要">看过猪跑很重要</h2><p>我当时下定决心要读研究生，这句话对我影响很大：“看过猪跑很重要”。</p><p>应该是阿里的毕玄在一次采访中说的。大概意思是公司的业务发展中，总会随着业务量出现各种新的问题。这时候能了解其他人的解法，是非常有参考价值的。</p><p>拿阿里来说，当时国内没有什么公司能作为参考，他们会去找谷歌的案例。但是找到的案例可能是10年前的，对于当时的业务量，足以够用。</p><p>而业务快速成长，遇到的困难也越来越多。到某一天，突然发现谷歌也没处理过现在阿里遇到的问题了。那这时候怎么办？</p><p>唯有自己解决了？</p><p>非也。这时候，答案在学术界。</p><p>时代变化，公司可能已经成长为巨头，此时不能直接照抄人家的答案，你也得作为开拓者参与最前沿的技术。</p><p>这时候各种国际会议、学术论文会进入你的视野，从学术角度找方案来落地才是最切实可行的。为什么说&quot;看过猪跑很重要&quot;，学习的知识来源是实践，而模仿是实践中效率最高的方式。</p><p>公司的业务变化，模仿的对象也要变化。当接近了金字塔的顶尖，所有人都不得不参与到名为学术的圈子里。</p><p>而这时候，你也得产出一些成果，可能是学术会议上的论文、各种分享会上的演讲，你也会变成别人眼中奔跑的猪。</p><p>我想读研究生，就是希望能先打个基础，以后做到这个程度的时候不会有短板。</p><h2 id="回炉重造的感受">回炉重造的感受</h2><p>自由、时间充裕、需要自控力。这三点是我最大的想法。</p><p>不用上班之后确实时间很多，也很自由，但是想做些什么有意义的事情，得靠自控力。</p><p>我每年都有立个目标和年末总结的习惯。</p><p>今年的目标，有两个：上学和变全栈。上学算是完成了，全栈还需要一点点时间，不过我相信我绝对没什么问题。</p><p>当时脑袋里萌发出读研的想法，我觉得这个决定可能会影响我一生，于是我马上订了去广州的高铁去找中介，并且立马签了合同。</p><p>现在已经初现倪端，我接触的技术范围大大超过了工作时。</p><p>我始终觉得方法比实际的努力更重要，因此我学习更注重学会学习方法，走通学习新知识的路，而不是学会这个知识本身。</p><p>在这方面，研究生读得还挺值得的。</p>]]></content>
    
    
    <summary type="html">港硕第一学期即将结束，赶due的期间摸鱼写个博客。</summary>
    
    
    
    <category term="生活" scheme="https://tulancn.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="杂谈" scheme="https://tulancn.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>秋招的总结</title>
    <link href="https://tulancn.github.io/2024/11/01/work/%E7%A7%8B%E6%8B%9B%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>https://tulancn.github.io/2024/11/01/work/%E7%A7%8B%E6%8B%9B%E7%9A%84%E6%80%BB%E7%BB%93/</id>
    <published>2024-11-01T07:08:20.000Z</published>
    <updated>2025-03-10T13:57:29.416Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经历">经历</h2><p>最开始是准备简历和找招聘信息，这部分就略过不谈。</p><p>麻烦的还是面试的准备，前期刷了leetcode的hot100，然后搞了一份80字面试宝典一点点看。</p><p>这些处理完已经到9月中旬了，这才慢慢投了一些厂子。</p><p>到十月国庆假期之后，才有一些笔试邀请过来。这时候已经是第二批次的秋招了，说实话互联网大厂的希望不大。</p><p>不过我也投了一个国企，还意外做了个行测。没怎么准备这种题目，说实话还是挺有意思的，感觉像智商测试。</p><p>等了一个月，到十月末，是华为给了面试的机会。</p><p>推荐人说粤港澳的面试安排有些混乱，想着在香港的同学线下跑去面试也不方便，也干脆给我安排走线上了。</p><p>一面在下午4点。</p><p>一开始是自我介绍，然后面试官挑项目经历问了些问题。</p><p>问了JNI是什么，为什么项目要用到JNI。回答提了一下Java的特性，JNI的用法和原理。</p><p>接着就继续从项目挑技术点问，中途话题被我带到了信创，就聊了不少信创的事情，感觉快变成纯聊天了。</p><p>一面大概是被安排了要问基础的问题，聊了一会儿之后面试官一拐话题说要继续问基础。</p><p>手撕挺简单的，给两个二进制字符串求和。这里我一边念题目一边说想法，然后再实现。</p><p>调试的时候出了一个bug，我说了一句Java不能多值返回就是麻烦，听到面试官笑了。</p><p>最后让我解释了一下调试时出现的问题原因，一面就结束了。</p><p>二面延后到了6点。</p><p>因为有些推迟了，面试官上来说直接做道题吧。</p><p>是一道分割整数数组的题，要求分割成三个数组，顺序不能变，数组和依次增大。问有几种解法。</p><p>还是一样，一边念题目一边说想法。写完面试官问了下几个优化的点。</p><p>然后就开始问项目，应该是想了解应聘者的技术思维吧。每个技术都问了下为什么这么做，有没有优化空间。</p><p>倒是都答上来了，有些比较复杂的回答就先把场景说明清楚。</p><p>中间提了项目中一些比较新奇的技术点，面试官也挺感兴趣的，就聊了挺久。</p><p>比较刁钻的就是问了下kafka有什么缺点。我一开始说不上来，就先扯了一句说它不信创，有些客户觉得可能影响项目的信创认证。然后才从技术角度回答了一下，主要是从不去中心化，对zk的强依赖（新版本用kraft去掉了），以及做消息防丢之后性能比较差这三个方面来说。</p><p>二面也顺利结束。</p><p>三面主管面是在9点。</p><p>上来主管先道歉，说拖了这么久，我说没关系。</p><p>跳过了项目拷打，主管说有工作经历相信技术上不会有问题。</p><p>先问为什么要重新去深造，会不会在华为工作以后也因为深造而离职。说上了研究生之后就像泄了气的皮球，没什么继续读书的兴趣。</p><p>接着问为什么选华为，我说我家里人都是花粉。面试官直接笑出声。</p><p>然后比较正式地回答是聊了下之前工作时做信创的经历，比较认可华为云自下而上从硬件层开始做信创的方案，认为公司有意愿去做其他公司不敢做的事，愿意在基础产品上投入。</p><p>举了个例子，之前在华为的服务器做压测，性能指标一直上不去。最后把JDK从openjdk换成华为的毕昇JDK，触发了软硬件协同，性能指标就一下子提上去了。</p><p>最后是关于压力，让我举个抗压的例子。先举了一个，不满意，然后再举。我说我为了读研究生考了7次雅思，第6次的时候崩溃大哭，但是收拾好心情继续去考了，最后终于过了。面试官边笑边说可以了。</p><p>反问环节，我说华为云内部是否很重视信创，给了肯定的回答。</p><p>结束时主管说希望还能再见到我，心理有预感已经过了。</p><p>晚上11点45准时收到面试的评价。</p><p>总体来说面试体验挺好的，虽然有些小插曲，但华为的员工们态度都很不错。</p><p>华为的面试之后，也就没有其他的面试流程了，秋招的拼搏阶段也算画上了句号。</p><h2 id="复盘">复盘</h2><p>如果要想一下还有什么能提高的，我觉得首先是自我介绍。</p><p>我发现很难在一两分钟内把自己所做过的所有事情都讲明白，一方面是经历比较丰富，一方面也是自己挑不到重点。</p><p>所以自我介绍还是得磨一磨，围绕项目把技术亮点重点说明，简单的经历就汇在一起用一句话带过。</p><p>第二个是下次春招要提前投递，这次秋招吃了不少的亏，主要是没有提前进行投递，导致没赶上9月的一批岗位。</p><p>最后是算法题，倒不是写不出来，就是我写得有点慢，这得再写一些题目才行。</p><h2 id="杂谈">杂谈</h2><p>仔细想想其实没面几家，算不上海投，算不上准备很充分，但大部分时候我也不就是这样轻轻松松就上了么。</p><p>就我个人而言，找工作这事情目前还不是特别急的一件事，秋招主要也是想打打基础。这么想的话就算这次最终结果不尽人意，可能也是可以接受的吧。</p>]]></content>
    
    
    <summary type="html">总结下这次秋招，没投几个简历，没费多大劲。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="找工作" scheme="https://tulancn.github.io/tags/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>大模型技术到底带来了什么</title>
    <link href="https://tulancn.github.io/2024/10/19/work/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E5%88%B0%E5%BA%95%E5%B8%A6%E6%9D%A5%E4%BA%86%E4%BB%80%E4%B9%88/"/>
    <id>https://tulancn.github.io/2024/10/19/work/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E5%88%B0%E5%BA%95%E5%B8%A6%E6%9D%A5%E4%BA%86%E4%BB%80%E4%B9%88/</id>
    <published>2024-10-19T08:33:05.000Z</published>
    <updated>2025-03-10T13:56:48.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正文">正文</h2><p>时间到2024年，以LLM为首的AI热潮似乎开始冷却。但是工具已经开始普及，大部分人已经开始尝试和AI一起工作。</p><p>我首先想提的是国内大模型的水平。目前接触下来，国产的AI在下限方面基本已经满足了我们的日常生活需要。也就是说，平时一些简单的任务已经可以让AI去完成，大部分问题都能收到一个令人满意的回答。对于大部分人的日常工作来说，已经足够。</p><p>如果要用一个角色来形容AI目前的水平，我觉得是一个知识面很广，能力很强，在经过适当调教能很好地完成任务的实习生。</p><p>这其实已经对我的日常工作方式有相当强烈地冲击了。举例来说，平日有些费时费力的总结报告，现在可以让AI来填写。一些技术文档全篇很长，而我需要知道的又只是其中的一个小点，那就可以让AI快速阅读之后我对其提问来获取答案。这种场景下，我的需求和问题都很明确，答案也有现成的，只是让AI在特定范围内进行一个快速的搜索，像是把LLM当做一个非常强大的搜索引擎，这种时候就非常好用。</p><p>还有种场景是内容的格式改写，比如数据改写为JSON格式。人工做很费力，但是AI能听懂指令快速生成结果。这就很好用。</p><p>那么工作的重点就变了。以前我的重点是把任务人工完成，要出力；现在我的重点是理清思路，转化为AI可以完成的任务，然后让AI先做一遍，我再检查一遍。这种思路确实就和分配任务给实习生是一样的。</p><p>所以，这就引出了LLM最适合的工作场景之一：<strong>重复性的、有明确工作规范和成果验收的工作</strong>。</p><p>另外，与AI协作的时候，很多人会感觉自己的能力也提高了。有一种场景，比如需要写一些自己平时不擅长的代码，像是后端程序员刚开始接触一门新的语言。这时候程序员可能是知道要做什么，但是苦于不知道怎么实现，因此需要一些额外的时间去学习。有了AI之后，这个学习成本降低了非常多，并且没有让人排斥的搜寻资料的过程。很多时候，我们要做的就是打开LLM输入界面，然后写出需求，等到LLM的回答，然后开始干活。得益于技术水平的提高，现在的LLM基本都很聪明，它们的回答大多数时候都是有用的。</p><p>感觉自己能力的提高后，会让人变得自信，然后就愿意去做一些以前不能做的事情。</p><p>但究其根本，不是人的能力提高了，而是<strong>学习的成本降低</strong>了。</p><p>这就是LLM最适合的第二个工作场景：<strong>快速让人在一个知识领域达到入门水平</strong>。</p><p>经过一些调教，我相信LLM在教育这方面大有可为。</p><p>然后我就要提GitHub Copilot。作为代码提示工具，有了LLM加持后，它聪明得让人欣喜。有人评价是，有了Copilot之后，编码的方式就变成了：写好注释，写好框架代码，然后停下来，等待Copilot填充。这确实是我的真实写照。</p><p>我觉得这就是大模型带来的最大影响：<strong>评价一个问题的难易程度，是取决于它有多少的部分能被LLM执行</strong>。</p><p>在一个任务中，我们总是倾向于让LLM做更多，而自己做更少。为什么这么说？第一个原因，LLM执行任务非常快，快到人类无法比拟的程度了；第二个原因，LLM执行任务的成本极低，几乎相当于没有成本。</p><p>但我们遇到的问题在最初往往并不是LLM可以直接解决的问题，因此我们要把问题转化为一个个LLM可以执行的最小单元。这考验了一个人思考问题、拆解问题和描述问题的能力，而显而易见的结论就是，这种能力越强的人，与LLM协作的能力也越强。</p><p>反过来，这其实也让我们每个人先专注于分析问题，而不是埋头就做。某种意义上来说，也算是反哺自身，让人去思考得更深入。</p><p>那么，我可以抛出一个命题：<strong>与大模型协作的能力越强，这个人的工作能力就越强</strong>。就像上文说的，一个人只有能分析拆解好问题，才能更好地让LLM完成工作。巧合的是，你把前面这句话中的LLM换成同事、下属也是一样成立的。原先AI部的同事就和我说过，常带实习生，或者手下有几个人的技术骨干，往往用Copilot也特别顺手。</p><p>在肉眼可见的未来，大家的工作中必定绕不开LLM，那么今早培养好自己相关的能力，可能也是条出路吧。</p><h2 id="总结">总结</h2><p>目前在工作和学习中，与大模型协作是非常常见的场景。</p><p>LLM的工作成本低，工作速度快，因此很适合进行一些重复性强、严格控制输入输出的工作。</p><p>LLM的语料资源丰富，能有效降低知识的学习门槛。</p><p>未来，人的工作能力可能很大程度上取决于他和AI配合能力的高低。</p>]]></content>
    
    
    <summary type="html">时至今日，LLM对我日常的工作和学习已经产生的极大的影响。聊聊我的思考。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="工作总结" scheme="https://tulancn.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>面试笔记-Java基础</title>
    <link href="https://tulancn.github.io/2024/10/16/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-Java%E5%9F%BA%E7%A1%80/"/>
    <id>https://tulancn.github.io/2024/10/16/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-Java%E5%9F%BA%E7%A1%80/</id>
    <published>2024-10-16T07:04:36.000Z</published>
    <updated>2025-03-10T14:00:22.699Z</updated>
    
    <content type="html"><![CDATA[<h2 id="语法基础">语法基础</h2><h3 id="Java语言特点">Java语言特点</h3><p>一次编译处处运行，天然具有跨平台的能力。只要不用JNI。</p><p>开源社区资源丰富，活跃。</p><p>适合企业级开发，多方面：有Spring为主流的开源框架，大型项目的构建和维护的成本较低。语言的规范性强。</p><h3 id="基本类型和包装类型">基本类型和包装类型</h3><p>byte、char、boolean、short、int、float、long、double</p><p>基本类型不是对象，包装类型是对象。使用时有自动的拆装箱机制。但编码过程中如果可以用基本类型的建议还是用基本类型。</p><p>区别在于对象是有对应的Class类，这些类信息会记录在JVM的ClassLoader中。对象有对象头。</p><p>字节码的方法签名，基本类型是BCJFI之类的大写字母，对象是L加全限定类名。</p><h3 id="instance-of">instance of</h3><p>检测对象是否是某个类型的实例。可以用来检测对象是否实现了某个接口。</p><p>HashMap中有使用到，用来检测当前节点的实现是链表还是红黑树。</p><h3 id="重写和重载">重写和重载</h3><p>重写建议加@Override注解，标明该方法是重写方法。</p><p>重写发生在子类重写了父类的同名且参数完全一致的方法，或者是实现了某个接口的方法。有重写就一定是有extend或者impliment。</p><p>重载是同名方法用不同的参数实现。同名方法的方法签名是不一样的。</p><h3 id="equals和">equals和==</h3><p>==是比较两个变量所指向的内存地址是否相同，其实是个指针操作。基本类型就相当于值比较。</p><p>equals是Object类的一个方法，Object类中的实现就是==。一般不同的类会有不同的实现，String的实现就是逐个比较字符值是否相等。</p><h3 id="四种引用">四种引用</h3><p>强引用：使用最多的引用，默认就是强引用。</p><p>软引用：缓存会用。可能会被回收，优先级在弱引用之后。</p><p>弱引用：young GC时必定会回收。</p><p>虚引用：这个引用不会影响GC，需要有其他引用方式指向这个对象。</p><h3 id="Cleaner">Cleaner</h3><p>Cleaner机制用到了虚引用。Cleaner继承了虚引用，GC后，所有的cleaner会被回收到一个pending队列，reference handler线程会调用这个pending队列中的对象，会检测下是不是Cleaner对象，是的话就直接调用Cleaner的clean方法。</p><h3 id="Exception和Error">Exception和Error</h3><p>都是Throwable的实现类。</p><p>运行时异常不用显式地catch。被检查异常需要catch或向上抛出。</p><p>Error是非常严重的错误，可能宕机。但是不需要显式抛出。</p><h2 id="数据结构">数据结构</h2><h3 id="HashMap">HashMap</h3><p>线程不安全。</p><p>put：hash之后找到数组中的对应位置，如果为空直接塞；不为空说明哈希冲突，检查一下当前的数据结构，按照数据结构来添加这个对象，可能是替换也可能是新增。</p><h3 id="LinkedList">LinkedList</h3><p>双向链表，线程不安全。</p><h3 id="阻塞队列">阻塞队列</h3><p>阻塞队列的经典实现：ArrayBlockQueue，LinkedBlockQueue</p><p>用了大小堆的priority queue</p><p>阻塞队列的锁就是线程池的锁。所以阻塞队列的性能也一定程度上决定了线程池的性能。举例：Disruptor、参考Disruptor实现了自己的阻塞队列。</p><p>阻塞队列可以当做对象池来用。</p><h2 id="JVM">JVM</h2><h3 id="JVM内存模型">JVM内存模型</h3><p>线程独占的：栈，本地方法栈，程序计数器</p><p>线程共享的：堆，方法区</p><p>栈：存储局部变量，操作栈，动态链接，方法出口。调用一个方法时入栈，方法返回出栈。</p><p>本地方法栈：Native方法的栈</p><p>程序计数器：当前程序执行的字节码位置。也就是打印异常堆栈时看到的执行到第几行出错。执行到native方法时，程序计数器清空，因为不是在执行字节码了。</p><p>堆：存储对象实例，线程共享。会垃圾回收。</p><p>方法区：虚拟机加载的类信息，常量，静态变量，JIT优化后的代码。总之就是些全局的信息。</p><h3 id="内存可见性">内存可见性</h3><p>也就是为什么要volatile关键字。</p><p>线程执行时，会拷贝一份线程间的共享变量至线程的工作内存。拷贝的数据可能已经被其他线程修改了，导致执行结果错误。</p><p>volatile会在共享变量修改时，强制同步一份副本至所有涉及到的线程的工作空间。</p><h3 id="类加载和卸载">类加载和卸载</h3><p>加载字节码文件到内存-&gt;验证并解析为Class类，创建静态变量执行静态代码块-&gt;实例化-&gt;GC</p><h3 id="三种加载器">三种加载器</h3><p>JAVA_HOME/lib: Bootstrap ClassLoader</p><p>JAVA_HOME/lib/ext: Extension ClassLoader</p><p>Application ClassLoader</p><h3 id="双亲委派">双亲委派</h3><p>加载器加载一个类时，先委托给父类加载器。父类无法加载才会自己加载。</p><p>主要是为了避免同一个类被多个加载器重复加载。已经避免Java的核心类被修改。</p><h3 id="回收算法G1">回收算法G1</h3><p>高频率回收，减少每次的停顿。</p><p>分老年代和年轻代。分块region，年轻代和老年代都由若干个分块组成。</p><p>年轻代使用并行的复制和收集算法。</p><p>mix-GC会回收一部分老年代。</p><p>标记清除算法：STW进行初始标记，确实GC root可直达的对象，伴随一次young GC。然后GC线程和应用线程并行进行并发标记，尽可能标记出存活对象，使用SATB记录对象的引用关系。最终标记，STW，修改并发标记的错误。然后多线程进行GC。</p><h3 id="ZGC">ZGC</h3><p>低延时垃圾收集器。</p><h3 id="Full-GC">Full GC</h3><p>当老年代满时会进行，耗时长。需要避免。</p><h3 id="对象分配原则">对象分配原则</h3><p>新对象优先在eden区，如果大于survivor的二分之一就认定为大对象，直接晋升老年代。</p><p>对象没存活过一次young GC，年龄+1，到达一定年龄（默认15）就晋升老年代。</p><p>Survivor区中同年龄对象的总和大小超过一半，这个年龄以及这个年龄以上的对象进入老年代。</p><h3 id="对象的创建过程">对象的创建过程</h3><p>先去常量池找类信息，然后加载类信息，找不到就报ClassNotFound。</p><p>为对象分配内存，将除了对象头之外的内存块初始化为0。</p><p>设置对象头。</p><h3 id="对象结构">对象结构</h3><p>对象头12字节，数组16字节。</p><p>对象信息，基础类型直接在对象内存存储。</p><p>Java会自动排列对象中的Field顺序以更好地满足8字节对齐。</p><h3 id="对象头的内容">对象头的内容</h3><p>4字节的hashcode</p><p>2字节的分代年龄</p><p>1字节偏向锁</p><p>1字节锁标志</p><p>4字节的对象类型指针，指向Class类信息</p><p>数组的话还有4字节的数组长度</p><h3 id="常见的调优工具">常见的调优工具</h3><p>jps：展示所有的java进程</p><p>jstat：查看虚拟机运行状态</p><p>jmap：主要用于dump</p><p>jstack：生成线程快照</p><p>jinfo：实时查看和修改JVM参数</p><p>MAT：dump文件分析</p><p>visualVM：自带可视化界面</p><p>arthas：阿里开源的运行时诊断工具</p><h2 id="多线程">多线程</h2><h3 id="怎么创建线程">怎么创建线程</h3><p>只有new Thread()才算创建了一个新线程。其他的方式比如实现Runnable接口Callable接口之类的都没有新线程产生。</p><p>Runnable接口Callable接口本质是一个task，需要提交给线程才能执行。</p><h3 id="如何停止一个线程">如何停止一个线程</h3><p>最佳是用退出标志，完成当前方法后当前线程终止。</p><p>推荐调用interrupt，会抛出interrupt异常。</p><p>stop可以强行终止，不推荐。</p><h3 id="notify和notifyAll有什么区别">notify和notifyAll有什么区别</h3><p>notify可能死锁，notifyAll不会。</p><p>假如有多个线程正在wait，会要求notify唤醒的任意一个线程可以处理接下来的逻辑，否则就会死锁。</p><p>如果无法正确处理，则应该继续notify下一个，并让自己进入wait状态。</p><p>notifyAll唤醒所有线程，但是争抢锁还是无序的。</p><h3 id="sleep和wait有什么区别">sleep和wait有什么区别</h3><p>sleep在线程类中，wait在Object类。</p><p>sleep不释放锁，会让出cpu。</p><p>wait会释放锁，并且让出cpu。调用wait后，会释放线程持有的所有对象锁，然后进入对象的等待区。只有针对此对象调用notify或notifyAll后，才会获取对象锁进入运行状态。</p><h3 id="volatile">volatile</h3>]]></content>
    
    
    <summary type="html">Java基础是纯纯的八股文了，但还是要过一遍。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="面试" scheme="https://tulancn.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>面试笔记-Spring</title>
    <link href="https://tulancn.github.io/2024/10/16/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-Spring/"/>
    <id>https://tulancn.github.io/2024/10/16/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-Spring/</id>
    <published>2024-10-16T03:43:17.000Z</published>
    <updated>2025-03-10T14:00:33.126Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spring篇">Spring篇</h2><p>Spring是一个Java开发框架，主要是解决企业级开发的一些痛点。一开始可能主要是用于Web应用的开发，但目前实践中它已经在国内Java后端开发这方面一统江湖。</p><p>它把各种业务逻辑都抽象封装为Bean，Spring负责项目启动时把这些Bean创建出来，进行业务开发的时候只需要通过Spring提供的API或其他方式，获取到对应的Bean就可以执行各种复杂业务操作。</p><h3 id="为什么是Spring">为什么是Spring</h3><p>轻量。</p><p>IOC，控制反转。业务开发不用关注对象创建之类的事情，只需要编码，并给出依赖，框架会自动帮你找到对应的实现类。</p><p>AOP，面向切面编程。</p><p>事务管理。</p><p>异常处理。</p><h3 id="Autowired和Resource的区别">Autowired和Resource的区别</h3><p>Autowired是Spring提供的注解，Resource是javax的注解。Spring是支持了javax提供的标准注解，是标准的一个实现。</p><p>假如换了框架（一般不会），Autowired必定不可用，Resource可能还能用。</p><p>Autowired默认按照type类装配，默认要求对象必须存在且唯一。可以用Qualifier来指定BeanName。</p><p>Resource注解提供了type和name两个注解字段。name的优先级高于type。</p><h3 id="依赖注入的方式">依赖注入的方式</h3><p>构造器、setter、接口。反射注入</p><h3 id="Spring-MVC">Spring MVC</h3><p>Spring MVC是Spring的一个模块，针对web应用的场景。</p><p>V不是普遍意义上的前端，而是用jsp或其他方式渲染的一个页面。</p><p>M是逻辑层，做计算和数据库操作。C是控制层，负责接收网络请求。</p><p>Spring在Servlet的基础上实现，定义了一个统一的请求接收入口：DispatcherServlet。DispatcherServlet会根据url和请求类型分发请求到用户编码的Controller中，然后执行用户的逻辑。</p><p>常用注解：@RequestMapping、@RequestBody、@ResponseBody</p><h3 id="AOP">AOP</h3><p>把一些业务无关，在系统层面共通的逻辑封装起来，从而减少代码冗余，有利于代码的扩展性和维护性。比如参数校验、日志采集、权限控制。</p><p>Spring的AOP是基于动态代理实现的。对于已经实现了接口的，用JDK动态代理，创建一个接口的实现类然后代理。没有实现的，用CGlib，创建一个类的子类然后代理。</p><p>也可以用AspectJ实现自己的AOP逻辑。</p><p>Spring的AOP属于运行时增强，AspectJ的AOP是编译时增强。AspectJ是字节码操作，性能稍高。</p><h3 id="关注点和横切关注点">关注点和横切关注点</h3><p>关注点是某个特定的业务功能，横切关注点是多个业务都会用到的系统功能，比如日志、参数校验、权限。</p><h3 id="什么是通知advice">什么是通知advice</h3><p>方法执行前和后要做的逻辑处理。</p><p>before</p><p>after</p><p>after-returning</p><p>after-throwing</p><p>around</p><p>这个结果处理和Spring WebFlux中的回调很像。</p><h3 id="IOC">IOC</h3><p>控制反转，把创建对象的权利移交给框架。</p><p>对象不需要程序自己去new，而是通过依赖注入从框架中获取。</p><p>IOC让组件保持松散的耦合，从而创造了可以进行AOP编程的余地。</p><h3 id="Spring-Bean生命周期">Spring Bean生命周期</h3><p>Servlet：实例化、初始化、接收请求、销毁。</p><p>Bean：</p><p>1、配置扫描，创建BeanDefinition，构建BeanFactory。</p><p>2、实例化：启动时的第一步。当BeanFactory被请求一个未实例化的Bean时，会调用createBean方法实例化一个Bean。</p><p>3、依赖注入：实例化之后的对象在BeanWrapper中，Spring通过BeanDefinition的配置进行依赖注入。这里会先找这个Bean的依赖所对应的BeanFactory，然后提供一个实例化Bean进行依赖注入。</p><p>4、处理Aware接口：BeanNameAware、BeanFactoryAware、ApplicationContextAware</p><p>5、BeanPostProcessor的postProcessBeforeInitialization方法</p><p>6、InitializingBean的afterPropertiesSet</p><p>7、BeanPostProcessor的postProcessAfterInitialization方法</p><p>8、scope为singleton的缓存在容器，prototype的返回最开始请求的客户端。</p><p>9、销毁，调用DisposableBean的afterPropertiesSet</p><h3 id="Bean的作用域">Bean的作用域</h3><p>1、singleton：单例</p><p>2、prototype：每个Bean一个实例</p><p>3、request：每个网络请求一个实例</p><p>4、session：每个session一个实例</p><p>5、globe-session</p><h3 id="Spring的设计模式">Spring的设计模式</h3><p>简单工厂模式：Bean的创建</p><p>单例模式：单例的Bean</p><p>代理模式：AOP的实现</p><p>适配器：Adapter，Spring中以Adapter结尾的一般都是适配器模式。比如AdvisorAdapter</p><p>观察者模式：Spring事件</p><p>模版模式：JdbcTemplate</p><h3 id="ApplicationContext和BeanFactory">ApplicationContext和BeanFactory</h3><p>BeanFactory是基础接口，只提供最基础的Bean管理功能，在spring-beans包中。</p><p>ApplicationContext继承了BeanFactory，并且扩展了事务管理、国际化、AOP。</p><p>BeanFactory是延迟加载，而ApplicationContext是启动时实例化所有的Bean。</p><h3 id="Spring的单例Bean是线程安全的吗">Spring的单例Bean是线程安全的吗</h3><p>是否线程安全和单例无关。Spring本身没有对Bean做任何并发的保护，这也不是Spring应该关心的东西。是否线程安全由开发者来保证。</p><h3 id="循环依赖怎么解决">循环依赖怎么解决</h3><p>三级缓存</p><p>第一级缓存是初始化完成的Bean；第二级是完成了实例化，但是没有完成依赖注入的Bean；第三级是没实例化的Bean，存放其BeanFactory。</p><p>假设有两个Bean A、B，AB循环依赖</p><p>1、Bean都是由BeanFactory创建，A的BeanFactory会先实例化A，放入二级缓存，随后进行依赖注入。</p><p>2、A的BeanFactory通过依赖找到了B，先从一级缓存找，一路找到三级缓存，最后找到了B的BeanFactory。这时候B没有创建，所以用B的BeanFactory创建B。BeanFactory会先创建一个B的实例，这样在二级缓存就有了一个B的实例，然后进行依赖注入。</p><p>3、B在依赖注入的时候会在二级缓存找到A，这时会直接获取A的实例注入B，这样就完成B的依赖注入。B完成创建后放入一级缓存，然后把B通过BeanFactory的方法返回给A，A就可以继续进行依赖注入。</p><h3 id="Spring事务隔离级别">Spring事务隔离级别</h3><p>同数据库的级别：读未提交、读已提交、可重复读、串行化</p><h3 id="Spring事务传播级别">Spring事务传播级别</h3><p>PROPAGATION_REQUIRED：有事务就加入，没有就创建</p><p>PROPAGATION_SUPPORTS：有就加入，没有就不加事务</p><p>PROPAGATION_MANDATORY：有就加入，没有就抛异常</p><p>PROPAGATION_REQUIRES_NEW：永远创建一个新的事务执行</p><p>PROPAGATION_NOT_SUPPORTED：永远不以事务执行</p><p>PROPAGATION_NEVER：有事务就抛异常</p><p>PROPAGATION_NESTED：有事务就创建嵌套事务，没有就创建</p><h3 id="Spring事务实现">Spring事务实现</h3><p>编程式事务、声明式事务。</p><p>PlatformTransactionManager中定义了事务的开始、提交、回滚等操作。不同数据库会有不同实现。</p><h3 id="Spring事务管理优点">Spring事务管理优点</h3><p>抽象了统一的接口。支持声明式事务管理。</p><p>可以和Spring多数据源结合。</p><h3 id="事务三要素">事务三要素</h3><p>数据源：事务的真正处理者。</p><p>事务管理器：处理事务的打开、提交、回滚。</p><p>事务应用和配置：表明哪些方法参与事务，事务的隔离级别，传播级别，超时时间。</p><h3 id="事务注解的本质">事务注解的本质</h3><p>@Transactional仅仅是一些元数据，这里通过AOP的方式，将元信息传递事务管理器，事务管理器再提交给数据源实现具体的事务。</p><p>其实就相当于在方法前后加了一些事务的代码。</p><h2 id="Spring-Boot篇">Spring Boot篇</h2><h3 id="为什么是Spring-Boot">为什么是Spring Boot</h3><p>独立运行：内嵌了tomcat、Jetty。不需要打成war包部署到容器。可以打成独立的Jar包运行。</p><p>配置简化，自动装配：能根据当前路径下的类、jar来自动配置bean。</p><p>无代码生成：配置过程中没有代码生成，没有xml配置文件，都是基于条件注解完成。</p><p>监控：默认就提供了很多监控端点。</p><p>Spring Boot大量使用了注解来驱动开发。</p><h3 id="核心注解">核心注解</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@SpringBootApplication</span><br><span class="line">@SpringBootConfiguration</span><br><span class="line">@EnableAutoConfiguration</span><br><span class="line">@ComponentScan</span><br></pre></td></tr></table></figure><h3 id="如何理解starters">如何理解starters</h3><p>一个把所有依赖都集成在一起的依赖包，简化引入的成本。</p><h3 id="如何在启动时执行一些特殊的逻辑">如何在启动时执行一些特殊的逻辑</h3><p>ApplicationRunner，CommandLineRunner。</p><p>ApplicationRunner有一个实现，是在启动时把程序的pid打印到一个文件中，方便运维。</p><h3 id="监视器-actuator">监视器 actuator</h3><p>actuator会对外暴露一些端点，可以通过配置的方式管理这些端点（endpoint）。这是能让用户来访问和监控程序当前的运行状态。配合运维平台使用。</p><h3 id="异常处理">异常处理</h3><p>ControllerAdvice，可以用来处理所有的异常，封装为统一的异常信息返回。</p><h3 id="配置加载顺序">配置加载顺序</h3><p>Spring Boot的配置加载顺序和其优先级一致。</p><p>1、命令行，–大于-D</p><p>2、Java系统属性，System.getProperties。</p><p>3、环境变量</p><p>4、yml文件</p><p>5、默认配置</p><h3 id="application和bootstrap文件">application和bootstrap文件</h3><p>老版本Spring Cloud Alibaba的Nacos只能使用bootstrap文件，后面版本修改了。</p><h3 id="自动装配">自动装配</h3><p>自动装配就是把第三方的Bean装载到Spring的容器里。以前是需要开发人员写xml才能把这些Bean装载到容器的。</p><p>原理：第三方组件定义了Configuration类，在其中声明了需要装载的Bean对象，同时也在这个类里定义了一些自动装配的规则。Spring在启动时通过META-INF/spring.factories文件找到对应的配置类，然后对这些配置类进行动态加载。</p><h2 id="Spring-Cloud篇">Spring Cloud篇</h2><h3 id="为什么是Spring-Cloud">为什么是Spring Cloud</h3><p>是用于构建分布式应用的开源框架。基于Spring Boot，提供与外部系统集成的能力。国内有很多二开的框架，比如Spring Cloud Alibaba，TSF，Spring Cloud Huawei。</p><h3 id="什么是微服务">什么是微服务</h3><p>一种架构。将单一功能的应用程序划分为一组小的服务，每个服务独立运行，服务之间相互协调、配合，最终实现一个业务功能。</p><p>服务之间使用轻量级的通讯机制（通常是http，但是dubbo之类的也可以）。</p><p>需要有一个中心来集中化管理这些服务。</p><h3 id="服务熔断和降级">服务熔断和降级</h3><p>分布式的场景下，某个服务出现异常，当检测到这种情况后，可以切断对该服务的调用。等到服务正常以后，再恢复服务。这就是熔断。</p><p>降级是是指服务熔断以后，如果还有对该服务的调用，直接失败。</p><h3 id="eureka-zookeeper-nacos">eureka zookeeper nacos</h3><p>Eureka的高可用机制比较完善，可以保证服务随时可用。</p><p>zookeeper对于多节点的数据一致性处理比较完善，可以用于主从选举。zk其实为了分布式协调而设计的，比如分布式锁、选举都可以用zk来做。</p><p>nacos提供了可视化界面，同时支持服务注册发现和配置下发，一个人干了两个人的活。</p><h3 id="Spring-Boot-Spring-Cloud">Spring Boot Spring Cloud</h3><p>Spring Boot是专注于开发单个微服务。</p><p>Spring Cloud是关注分布式系统的，分布式系统可以是多个Spring Boot开发的微服务的集合，Spring Cloud用于提供微服务之间调用、微服务的配置获取、服务注册、路由等。</p><h3 id="负载均衡">负载均衡</h3><p>某个服务可能有多个实例，调用该服务时，每个请求会分发到不同的实例处理，有助于提高服务的吞吐量。</p><h3 id="Feign">Feign</h3><p>Feign是一个HTTP请求客户端，提供了声明式的调用方式。主要是通过JDK动态代理实现。它整合了负载均衡的能力（Spring Cloud LoadBalance）</p>]]></content>
    
    
    <summary type="html">这里记录一下面试的准备过程中关于Spring的笔记。问Java不问Spring简直不可能好吧。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="面试" scheme="https://tulancn.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>面试笔记-数据库</title>
    <link href="https://tulancn.github.io/2024/10/14/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>https://tulancn.github.io/2024/10/14/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E5%BA%93/</id>
    <published>2024-10-14T05:48:06.000Z</published>
    <updated>2025-03-10T14:00:06.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点">知识点</h2><h3 id="三范式">三范式</h3><p>第一范式：列不可再分；</p><p>第二范式：表中的每一行都可以唯一区分，通过主键实现；</p><p>第三范式：（用于避免数据冗余，减少内存占用，实践中常被忽视）表的非主键字段不能依赖于其他非主键字段；比如学生表，里边有班级名称字段和辅导员名称字段，那么班级和辅导员会大量重复，应当拆分出班级表和辅导员表。</p><h3 id="MySQL的存储引擎">MySQL的存储引擎</h3><p>MyISAM、InnoDB、Memory</p><p>MyISAM：全表锁，没有事务和外键，单表执行性能高，并发性能差，空间占用小。</p><p>InnoDB：行锁，有事务，支持自增序列，支持外键，并发性能高，空间占用大。</p><p>Memory：纯内存。</p><h3 id="InnoDB和MyISAM">InnoDB和MyISAM</h3><p>InnoDB支持事务，默认是每一条SQL都封装为一个事务。MyISAM没有事务。</p><p>InnoDB有外键，MyISAM没有。</p><p>InnoDB是行锁，MyISAM是表锁，InnoDB并发场景性能会好一些。</p><p>InnoDB是聚集索引，必须有主键。在表上建立的其他索引，都是指向主键。通过其他索引查询时，先经过索引查到主键，再通过主键查找数据。MyISAM是非聚集索引，主键和索引都是直接指向数据指针，主键和索引相互独立。</p><p>InnoDB不保存表的行数，计算行数时要全表扫描。MyISAM保存了表的行数。</p><h3 id="数据库事务">数据库事务</h3><p>ACID：atomic、consistency、isolation、durability</p><p>原子性：一个事务内的多个操作可以看成一个原子操作，即要么成功要么失败。事务中任意一个操作失败了都会回滚到没有开始事务之前的状态。</p><p>一致性：事务操作的结果和业务规则是一致的。即事务成功时，可以达成操作者目的。</p><p>隔离性：多个事务之间互相数据隔离，彼此没有影响。</p><p>持久性：事务完成后会持久化到数据库。</p><h3 id="索引是什么">索引是什么</h3><p>官方：一种帮助MySQL快速获取数据的数据结构。</p><p>类似目录，默认是B+树实现。</p><h3 id="索引的优缺点">索引的优缺点</h3><p>优点：提升查询速度。</p><p>缺点：索引也有空间占用，更新速度变慢，因为表更新的时候索引也要更新。</p><h3 id="SQL优化">SQL优化</h3><p>原则：避免全表扫描，查询时尽量走索引。少于三张表的查询允许关联，关联不了就走子查询。</p><p>1、尽量不要用select *</p><p>2、减少子查询，用少于三张表的关联查询替代</p><p>3、少用in和not in，因为绝对可以用exists和not exists替代</p><p>4、where子句中慎用!=，走不到索引</p><p>5、少用or，走不到索引</p><p>6、少用null判断，走不到索引</p><h3 id="drop、delete和truncate">drop、delete和truncate</h3><p>drop和truncate不能回滚，不在事务中。delete是在事务中，提交时才生效。</p><p>drop删的最多，会删表结构，可以重新建同名表，及时生效不在事务中。</p><p>truncate其次，对比drop是少删表结构，仅删除数据，还是不在事务中。</p><p>delete再次，仅删除表数据，会在事务中，事务提交后才生效。</p><h3 id="视图">视图</h3><p>理解为虚拟表。类似函数封装的概念。</p><p>可以封装复杂查询的结果。</p><p>可以用来做权限隔离，比如仅展示某几个字段给特定用户。</p><p>底层表有改动，比如拆分了，可以建一个视图，避免上层应用修改SQL。</p><h3 id="并发事务的问题">并发事务的问题</h3><p>脏读：一个事务读到了另一个事务修改了但未提交的数据。</p><p>修改丢失：两个事务同时修改了同一个值，后一个覆盖了前一个。比如两个事务同时做x-1，先读x再减一，并发之后可能结果是x-1而不是x-2。</p><p>不可重复读：一个事务未结束时，读到了另一个事务B修改了并提交的最新值。</p><p>幻读：一个事务查询一批数据的过程中，这批数据被另一个事务新增或删除了一些数据（比如新增了几条数据），于是查询到了新的数据。</p><p>不可重复读重点在修改，幻读重点在新增或删除。重点在行锁和表锁区别。</p><h3 id="事务隔离级别">事务隔离级别</h3><p>MySQL的InnoDB默认是可重复读</p><p>读未提交：脏读，不可重复读，幻读</p><p>读已提交：不可重复读，幻读</p><p>可重复读：幻读（解决了单行的修改，但没解决表级别的增删）</p><p>串行化：事务逐个执行，库锁</p><p>大部分数据库是读已提交级别的事务（比如postgre）。</p><p>mysql额外用了next key Lock，用行锁和间隙锁在可重复读的场景下避免了幻读。具体做法就是，锁了一行数据的前一条和后一条中间的范围。比如5、10、15三条数据，当10被事务读的时候，如果有一个事务插入一条数据8，则会触发间隙锁，插入8的事务会阻塞。这里和聚簇索引也有关系。</p><h3 id="大表优化">大表优化</h3><p>方向：单体变分布式-拆，限制结果数量</p><p>1、查询时限制范围，分页</p><p>2、读写分离，拆到多个机器上</p><p>3、垂直拆分，把字段拆到不同的表中</p><p>4、水平拆分，根据某些逻辑把数据分到不同的片区。比如地区信息做异地多活。</p><h3 id="分库分表后的ID处理">分库分表后的ID处理</h3><p>可转化为全局唯一ID问题：与消息中间件的消息防丢差不多。</p><p>uuid、雪花、美团的leaf。</p><h3 id="MySQL的一条查询SQL执行过程">MySQL的一条查询SQL执行过程</h3><p>8.0没有查询缓存。</p><p>1、语法分析：包括解析SQL语法的正确性，涉及到的表和字段是否存在</p><p>2、准备阶段：会对查询进行逻辑优化，比如选择索引，去除不必要的子查询之类的。逻辑优化后还有物理优化，会根据统计信息和成本计算模型来确定具体的执行计划</p><p>3、执行查询：由执行引擎和存储引擎交互产出查询结果</p><p>4、结果返回：去重排序之类的处理，最终结果返回给客户端</p><h3 id="varchar和char的区别">varchar和char的区别</h3><p>varchar可变长度，char不可变，定长。</p><p>比如varchar(30)的字段存一个字节长度为3的字符，实际空间占用可能为5或者6字节左右，因为有一定的空间用于存储其字段长度。</p><p>char(30)的字段存同样的字符，那么固定占用30字节，后面空的会补零。</p><p>char读写快，空间占用大。varchar省空间，读写慢。</p><h3 id="int-11-的11含义">int(11)的11含义</h3><p>不影响字段的存储范围，仅影响查询的展示效果。</p><h3 id="MySQL的索引类型">MySQL的索引类型</h3><p>主键索引：必须唯一，不可为空</p><p>普通索引：允许重复和空</p><p>唯一索引：值必须唯一，可以为空</p><p>全文索引：只能在文本字段创建，用于加速like查询</p><p>空间索引</p><p>前缀索引</p><p>单列和组合索引：最左匹配原则，优先用组合索引</p><h3 id="什么场景不适合索引">什么场景不适合索引</h3><p>1、经常更新的列</p><p>2、内容大量重复的列</p><p>3、表记录太少（不清楚具体是多少）</p><p>4、写远大于读的表</p><h3 id="MVCC">MVCC</h3><p>多版本并发控制。用来解决读写冲突，是无锁并发。事务分配事务号（单向递增），每个修改保存一个版本。读只读老版本的数据，修改在新版本。可以同时避免脏读和不可重复读。</p><p>实现：</p><p>1、每行数据有一个版本链</p><p>2、事务id，单调递增</p><p>3、可见性判断，根据事务级别来判断是否可见</p><p>4、过期数据清理</p><p>级别和判断逻辑：</p><p>读已提交：单行数据版本号小于当前事务版本号，并且此事务已提交，则可以看到</p><p>可重复读：创建一致性视图，包含事务开始时所有可读的事务号。</p><p>缺点，做不到串行化执行，在解决写冲突的时候还是要用锁或者重做</p><h3 id="MySQL锁">MySQL锁</h3><p>读锁和写锁（共享和排他）。</p><p>按照粒度，可以划分为表锁和行锁。MyISAM是表级别的读写锁，InnoDB支持行锁。</p><h3 id="锁升级">锁升级</h3><p>行锁只能在索引上，没索引自动升级成表锁。</p><p>没走索引时也会升级成表锁。比如非唯一索引相同的内容超过表的一半，优化器会选择不走索引。</p><h3 id="乐观锁和悲观锁">乐观锁和悲观锁</h3><p>悲观锁：倾向于一行数据总是会被同时修改，所以修改前加上排他锁</p><p>乐观锁：倾向于一行数据不一定会被同时修改。加上版本号，类似MVCC，修改前读取版本号，修改后版本号+1，在更新版本号时做版本号检查，如果版本号和预期不符，说明此时有其他人修改了数据，否则就直接更新。乐观锁一般配合自旋，形成一个类似CAS的操作。</p><h3 id="避免死锁">避免死锁</h3><p>获取锁一定要加超时时间；</p><p>按照固定顺序获取资源；</p><p>事务尽量简短；</p><p>事务隔离级别低一些；</p><p>避免事务中的用户交叉；</p><h3 id="索引注意事项">索引注意事项</h3><p>1、where中使用!=、&lt;&gt;、or会导致放弃使用索引</p><p>2、符合索引要复合最左前缀原则</p><p>3、where中进行字段表达式运算、函数运算可能导致索引失效</p><p>4、Like使用是%不能在前，模糊匹配可以用全文索引</p><p>5、字段是字符串类型，必须加引号，否则索引失效</p><h3 id="主键-索引">主键 索引</h3><p>主键是唯一索引，唯一索引不一定是主键</p><p>主键不允许空，唯一索引可以为空</p><p>一个表只能有一个主键，但可以有多个唯一索引</p><p>主键是约束，而唯一索引是冗余的数据结构</p><h3 id="MySQL的高可用">MySQL的高可用</h3><p>MySQL分库分表。</p><p>意义不是很大，一般是整个系统都做高可用。</p><h2 id="总结">总结</h2><p>MySQL的MVCC机制是一个非常标准的实现，可以和我之前做的缓存框架结合起来，很多道理都是共通的。</p>]]></content>
    
    
    <summary type="html">这里记录一下面试的准备过程中关于数据库的笔记。原先工作中对数据库的使用实在是太少了...不过还好大部分东西都还记得，复习一下就很快理解了。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="面试" scheme="https://tulancn.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>面试笔记-网络协议</title>
    <link href="https://tulancn.github.io/2024/10/14/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"/>
    <id>https://tulancn.github.io/2024/10/14/work/tips/%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0-%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/</id>
    <published>2024-10-14T05:41:52.000Z</published>
    <updated>2025-03-10T14:00:16.272Z</updated>
    
    <content type="html"><![CDATA[<h3 id="HTTP响应码">HTTP响应码</h3><p>200 - 成功</p><p>301 - 永久重定向    302 - 临时重定向</p><p>4开头是客户端问题</p><p>400 - 客户端请求有问题  404 - 找不到对应资源，url错了</p><p>5开头是服务器问题</p><p>500 - 服务器崩了  504 - badgateway</p><h3 id="Forward和Redirect">Forward和Redirect</h3><p>Forward是服务器把请求转发给其他服务器或应用处理，处理完原路返回给客户端</p><p>Redirect是服务器直接返回指令，让客户端重新请求其他服务器 （去买牛肉，到店了发现店面贴着告示，说牛肉店搬到另一条街了，所以要去新的地址买）</p><h3 id="GET和POST">GET和POST</h3><p>用途不同，GET用来获取资源，POST用来提交数据，体现在浏览器的标签只能收藏GET请求，不能收藏POST请求</p><p>参数：GET一般把参数直接拼在URL，POST在消息头或消息体</p><p>编码：GET和URL一致，POST无所谓，编码按照约定来</p><h3 id="TCP和UDP">TCP和UDP</h3><p>TCP先建立连接才能通讯，可靠，传输性能一般</p><p>UDP无连接，不可靠，传输性能高</p><p>TCP是点对点，UDP支持广播</p><p>但UDP也不是绝对不可靠，HTTP3的底层换成了QUIC协议，是基于UDP实现的</p><h3 id="HTTP和HTTPS">HTTP和HTTPS</h3><p>HTTP用80端口，HTTPS用443</p><p>HTTPS多了加密，有性能开销，且加密证书需要额外购买</p><h3 id="HTTP、TCP和Socket">HTTP、TCP和Socket</h3><p>Socket是网络协议的API，由应用层的程序或编程语言提供</p><p>用Socket可以实现一个HTTP协议的客户端或服务端</p><p>HTTP是应用层的协议，基于TCP来实现</p><p>TCP是网络层协议</p><h3 id="HTTP的长连接和短连接">HTTP的长连接和短连接</h3><p>本质是TCP的长连接和短连接。消息头里有个字段是keep-alive，如果为true，则一次请求后之前建立的TCP连接还会保留，不会进行四次挥手，下次再有请求就复用这个连接。</p><p>在同一个客户端会有大量访问的情况下能提升性能，但是对服务器的连接数有要求。所以有了NIO，比如Netty，能支持大量的客户端连接。</p><p>保持长连接的时间由服务端进行控制。tomcat有配置。</p><h3 id="TCP三次握手">TCP三次握手</h3><p>服务端在某个端口开启监听，客户端通过ip+port向服务器发起连接请求。</p><p>第一次：客户端-&gt;服务端，syn=1，seq=x</p><p>第二次：服务端-&gt;客户端，syn=1，seq=y，ack=x+1。第二次表示客户端到服务端的链路已经通了，这时候要验证服务端到客户端的链路是否通。</p><p>第三次：客户端-&gt;服务端，seq=x+1, ack=y+1。告知服务端，你发送的消息我也能收到。</p><p>为什么要三次：因为是双工通讯，要确认消息序号的起始值。</p><h3 id="TCP四次挥手">TCP四次挥手</h3><p>主动断开的一方，发送FIN，告知不再发送消息；对端接收到，返回ack；</p><p>此时对端还可以发送消息，因为对端可能还有消息没传完。</p><p>对端传完之后对端发送FIN，告知不再发送消息；主动断开的一方返回ack，连接彻底断开。</p><h3 id="TCP粘包">TCP粘包</h3><p>本质是因为TCP是传输字节流，字节流是没有边界概念的。</p><p>包的概念是怎么来的？字节流实际在TCP传输的过程中是会分成多个数据包依次传输，这里的数据包切割方式和上层应用无关，因此这些数据包很可能和上层应用的协议包对不上。比如上层应用发了两个请求，但是底层会划分为3个数据包来传输。</p><p>TCP的分包是由滑动窗口来控制的。</p><p>解决方式，都是在应用的协议层解决：</p><p>1、最常用，在协议包里定义协议头，协议头是固定格式的，在协议头里再定义一个协议体长度的变量。HTTP等协议都是如此。</p><p>2、把协议固定长度，所有协议包的长度都一致。</p><p>3、特殊分隔符号。</p><h3 id="TCP可靠性">TCP可靠性</h3><p>最重要的就是序列号和确认号（ack）：包里带seq，接收端检测包的完整性，完整则返回ack。</p><p>超时重传：发送端发了包以后开始计时，如果一定时间内没收到ack，则重传。</p><p>重排序：多个数据包在传输过程中可能乱序，tcp收到后对数据包重排序，保证上层读取的字节流是和发送端传的一致。</p><p>丢弃重复包：比如超时重传等情况会造成重复包，tcp会丢弃这些重复的。</p><p>流量控制：滑动窗口来控制流量，避免发送过快接收端无法处理。（行情客户端遇到过）</p><h3 id="OSI七层模型">OSI七层模型</h3><p>应用层：HTTP、FTP</p><p>表示层：加解密</p><p>会话层：RPC</p><p>传输层：TCP、UDP</p><p>网络层：IP、IPv6</p><p>数据链路层：物理寻址层。交换机等</p><p>物理层：硬件</p><p>各种RPC框架一般是涵盖了会话层以下的所有能力，业务框架可能还会涵盖表示层。</p><h3 id="浏览器输入一个网址后发生了什么">浏览器输入一个网址后发生了什么</h3><p>1、域名-&gt;IP的转换，会经过浏览器缓存、系统缓存、hosts配置文件、路由器缓存，搜索域名对应的服务器。</p><p>2、建立TCP连接</p><p>3、发送HTTP的GET请求</p><p>4、请求经过路由器转发到服务器</p><p>5、服务器处理请求，返回网页文件（HTML）</p><p>6、浏览器渲染html（渲染过程中执行了js，可能产生新的HTTP请求）</p><h3 id="如何跨域">如何跨域</h3><p>浏览器执行js的时候，如果产生新的HTTP请求，且新的请求和当前网址不一致，则发生了跨域，会被拒绝。</p><p>CORS可以，但是需要服务器和浏览器都支持才行。</p><p>使用NGINX反向代理，浏览器不用做任何支持，更合适。原理也是CORS，只是在nginx这一层改了请求。</p><h3 id="HTTP-1-0-1-1-2-0-3-0">HTTP 1.0 1.1 2.0 3.0</h3><p>1.0是无状态无连接，也就是每次请求都建立一次tcp连接。</p><p>1.1添加了connection:keep-alive，可以建立长连接。但是要求服务器必须对应所有请求的顺序返回响应。</p><p>2.0 添加了数据流，gRPC有使用。服务器可以并行传输数据，因为每个流有自己的streamid和序号。但所有流用的是一个TCP连接。</p><p>2.0 还做了头部压缩，通过让服务器和客户端都缓存请求头的field表来实现。</p><p>3.0 底层使用QUIC，不再使用TCP，QUIC是基于UDP实现的。在UDP上实现了TCP的可靠传输能力。</p><h3 id="HTTP与TCP-IP">HTTP与TCP/IP</h3><p>HTTP是应用层的报文协议，一般数据传输是用TCP/IP实现。</p><p>TCP传输的是字节流，没有对包的格式做要求。在此基础上，HTTP定义了请求包的格式。</p><p>理论上可以把HTTP的包用其他传输协议来发送。</p><h3 id="HTTP长连接短连接">HTTP长连接短连接</h3><p>本质是TCP的长连接和短连接。HTTP的1.1定义了Connection:keep-alive的请求头字段，避免每次请求都要三次握手和四次挥手。</p><h3 id="长连接和短连接的优缺点">长连接和短连接的优缺点</h3><p>长连接对于活跃的客户端，能减少连接和断连的成本。但是客户端一多，服务器资源比较浪费，可以用NIO来避免这个问题，一个线程处理多个TCP连接。</p><p>短连接对服务端的管理比较简单，但是qps上去之后在连接和断连上会浪费很多时间和带宽。</p><h3 id="域名解析过程">域名解析过程</h3><p>从开销最小的地方开始找，没有就去更远的地方找。</p><p>1、本机：浏览器缓存、操作系统缓存、hosts文件，</p><p>2、本地配置的DNS服务器，可能是学校、企业的，如果缓存中有，直接返回。</p><p>3、本地配置的DNS服务器没有，则其会向更高的根域名服务器查询，获得对应的权威域名服务器地址，然后再请求权威域名服务器获得IP。获得IP后，本地的DNS服务器会返回给本机，并缓存该域名，下一次再查询就直接从本地的DNS服务器可以查到。</p><p>4、浏览器、操作系统也可能会缓存这个域名。</p>]]></content>
    
    
    <summary type="html">这里记录一下面试的准备过程中关于网络协议的笔记。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="面试" scheme="https://tulancn.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>JRE环境使用Arthas</title>
    <link href="https://tulancn.github.io/2024/09/20/work/tips/JRE%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8Arthas/"/>
    <id>https://tulancn.github.io/2024/09/20/work/tips/JRE%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8Arthas/</id>
    <published>2024-09-20T11:12:39.000Z</published>
    <updated>2025-03-10T14:00:46.667Z</updated>
    
    <content type="html"><![CDATA[<h2 id="场景">场景</h2><p>实际工作中常遇到一种情况，线上环境出现异常，但是环境安装的是JRE。</p><p>JRE环境下没有了JDK提供的各类工具，根本无法排查问题。如果重装JDK，那么同时也得重启项目，而项目一旦重启，问题的环境就已经丢失，下一次再遇到同样的问题就可遇不可求了。</p><h2 id="解决方案">解决方案</h2><p>使用轻量级的程序附加工具jattach可以做到将Arthas的Jar包attach到特定的进程上。</p><p>网上有类似的解析。最关键的一点是Arthas只需要将一个jar附加到JVM中就可以运行了。</p><p>这里提供一个脚本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 获取当前脚本所在的目录</span><br><span class="line">script_dir=$(cd &quot;$(dirname &quot;$&#123;BASH_SOURCE[0]&#125;&quot;)&quot; &amp;&amp; pwd)</span><br><span class="line"></span><br><span class="line"># 用户输入进程ID</span><br><span class="line">read -p &quot;请输入进程ID: &quot; pid</span><br><span class="line"></span><br><span class="line"># 用户输入可选参数host，提供默认值</span><br><span class="line">read -p &quot;请输入主机地址和端口号（格式：IP 端口，默认为 127.0.0.1 3658）: &quot; host</span><br><span class="line">host=$&#123;host:-&quot;127.0.0.1 3658&quot;&#125;</span><br><span class="line"></span><br><span class="line"># 检查当前目录下是否存在jattach文件</span><br><span class="line">if [[ ! -e &quot;$script_dir/jattach&quot; ]]; then</span><br><span class="line">  # 查找以jattach-linux开头的tgz压缩包</span><br><span class="line">  jattach_tgz=$(find &quot;$script_dir&quot; -maxdepth 1 -type f -name &quot;jattach-linux-*.tgz&quot;)</span><br><span class="line"></span><br><span class="line">  if [[ -z &quot;$jattach_tgz&quot; ]]; then</span><br><span class="line">    echo &quot;当前目录下未找到名为jattach的文件或符合条件的jattach-linux-*.tgz压缩包。&quot;</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  # 判断系统架构</span><br><span class="line">  system_arch=$(uname -m)</span><br><span class="line">  case $system_arch in</span><br><span class="line">    &quot;aarch64&quot;|&quot;arm64&quot;)</span><br><span class="line">      arch=&quot;arm64&quot;</span><br><span class="line">      jattach_file=&quot;jattach-linux-arm64.tgz&quot;</span><br><span class="line">      ;;</span><br><span class="line">    &quot;x86_64&quot;|&quot;amd64&quot;)</span><br><span class="line">      arch=&quot;x64&quot;</span><br><span class="line">      jattach_file=&quot;jattach-linux-x64.tgz&quot;</span><br><span class="line">      ;;</span><br><span class="line">    *)</span><br><span class="line">      echo &quot;当前系统架构不支持自动解压jattach工具。&quot;</span><br><span class="line">      exit 1</span><br><span class="line">  esac</span><br><span class="line"></span><br><span class="line">  # 检查对应的jattach压缩包是否存在</span><br><span class="line">  if [[ ! -f &quot;$script_dir/$jattach_file&quot; ]]; then</span><br><span class="line">    echo &quot;当前目录下未找到与系统架构匹配的$jattach_file压缩包。&quot;</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  # 解压缩对应架构的jattach工具到当前脚本目录，并使用绝对路径</span><br><span class="line">  tar -xzvf &quot;$script_dir/$jattach_file&quot; -C &quot;$script_dir&quot;</span><br><span class="line"></span><br><span class="line">  # 检查解压后jattach是否成功生成</span><br><span class="line">  if [[ ! -e &quot;$script_dir/jattach&quot; ]]; then</span><br><span class="line">    echo &quot;解压失败或未能在当前目录找到解压后的jattach文件。&quot;</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 使用jattach命令加载arthas-agent.jar，使用绝对路径</span><br><span class="line">&quot;$script_dir/jattach&quot; $pid load instrument false &quot;$script_dir/arthas-agent.jar&quot; &amp;&amp; \</span><br><span class="line"></span><br><span class="line"># 运行arthas客户端，同样使用绝对路径</span><br><span class="line">java -jar &quot;$script_dir/arthas-client.jar&quot; $host</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最关键的就是脚本中最后的两句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 使用jattach命令加载arthas-agent.jar，使用绝对路径</span><br><span class="line">&quot;$script_dir/jattach&quot; $pid load instrument false &quot;$script_dir/arthas-agent.jar&quot; &amp;&amp; \</span><br><span class="line"></span><br><span class="line"># 运行arthas客户端，同样使用绝对路径</span><br><span class="line">java -jar &quot;$script_dir/arthas-client.jar&quot; $host</span><br></pre></td></tr></table></figure><p>这其实是一个命令，分成了两行。第一个命令是将<code>arthas-agent.jar</code>加载到某个pid的程序中，第二个命令是运行arthas的界面。</p><p>脚本提供了自动安装jattach的能力，只需要将jattach的安装包与此脚本方式在arthas的目录内，然后运行此脚本即可。</p>]]></content>
    
    
    <summary type="html">Arthas是线上故障排查、性能调优常用的工具。这里提供一个在JRE环境也可以使用Arthas的方式。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="优秀实践" scheme="https://tulancn.github.io/tags/%E4%BC%98%E7%A7%80%E5%AE%9E%E8%B7%B5/"/>
    
  </entry>
  
  <entry>
    <title>工作中印象最深刻的一件事</title>
    <link href="https://tulancn.github.io/2024/09/16/work/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%8D%B0%E8%B1%A1%E6%9C%80%E6%B7%B1%E5%88%BB%E7%9A%84%E4%B8%80%E4%BB%B6%E4%BA%8B/"/>
    <id>https://tulancn.github.io/2024/09/16/work/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%8D%B0%E8%B1%A1%E6%9C%80%E6%B7%B1%E5%88%BB%E7%9A%84%E4%B8%80%E4%BB%B6%E4%BA%8B/</id>
    <published>2024-09-16T12:02:30.000Z</published>
    <updated>2025-03-10T13:56:43.120Z</updated>
    
    <content type="html"><![CDATA[<h2 id="技术之外的东西">技术之外的东西</h2><p>如果是放在还没参加工作时，看到这个问题“你在项目中遇到过的印象最深刻的一件事”，我大概率就是：胡诌一个很难的技术问题，然后说这问题解决的过程有些困难，但是经过我艰苦卓绝地努力，还是完成了，最后觉得自己很有成就感巴拉巴拉……</p><p>但是放到今天，再看这问题，我就倾向于带一些技术之外的东西进去。</p><p>这能聊的就多了。</p><p>比如有个不怎么开窍的实习生，怎么教都教不会，最后是我自己帮他写了代码。我反思之后，觉得自己带人的方式有些问题，从此之后就换了一种方式教人。如果实习生能力差，就选取一些时间上限制不那么多的任务；如果愿意挑战，就分一些预研性质的任务，让他产出方案。<br>这样能说的就多多了，我相信也是大部分面试官可以感同身受的。</p><p>当然上面只是个例子，最主要的一点就是，我觉得和同事相处的方式、选取某种技术的决断、一些任务优先级的取舍，这些都可以是印象深刻的事情。对于这些事情，技术固然包含在其中，但提升了这些事难度的、给人深刻印象的，恰恰就是技术之外的东西。</p><h2 id="印象最深的一件事">印象最深的一件事</h2><p>然后就提到我自己参加工作后印象最深的一件事，应该就是写公司业务协议序列化框架的纯Java版本了。</p><p>准确说起来，这块的代码我没有写多少。我主要是负责这个任务的分配、监督和技术指导，编码由组内的一个同事负责。</p><p>前期预研时，阅读了Protobuf的源码，然后实现时是参考了C++的实现。</p><p>简单来说，就是参考C++的代码，从Protobuf中找到对应的可用代码，然后移植到我们的框架中。</p><p>包体的解析逻辑有大量的可用实现，但是Java对象属性的获取和设置的逻辑，我们是用Unsafe重新写了一遍。</p><p>整个过程顺利得不可思议，原计划两个月的任务，最后一个月就完成了。</p><p>但是问题就出在这个完成之后，我们发现还有一些性能优化的空间。</p><p>这个同事有些钻牛角尖，希望能把整个代码重构，然后做性能优化。但是我组织了他。</p><p>这就是我印象最深的地方了，我经过了取舍，决定把重构和优化的工作往后无限期推迟，让他先基于目前的版本给出详细的文档。</p><p>我们大部分同事都不具备给项目收尾的能力，就是一个模块，写到什么程度，我们就应该停止迭代，转而发布正式版。</p><p>这个界定是有些困难的，作为开发者，你永远不会满意，永远会觉得还有优化空间。</p><p>但工作都是由优先级的，我们只能接受不完美的现实，把有限的精力投入更高优先级的事项中去。</p><p>这就是我从这件事情里学到的。</p>]]></content>
    
    
    <summary type="html">在准备秋招面试，看到这个常见的面试题，突然有感而发。工作之后，发现这个问题只从技术角度看，实在是有些肤浅，或许可以提出更多的东西。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="工作总结" scheme="https://tulancn.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Netty的FastThreadLocal带来了什么？</title>
    <link href="https://tulancn.github.io/2024/09/13/work/java/Netty%E7%9A%84FastThreadLocal%E5%B8%A6%E6%9D%A5%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <id>https://tulancn.github.io/2024/09/13/work/java/Netty%E7%9A%84FastThreadLocal%E5%B8%A6%E6%9D%A5%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/</id>
    <published>2024-09-13T06:49:51.000Z</published>
    <updated>2025-03-10T13:59:46.997Z</updated>
    
    <content type="html"><![CDATA[<h2 id="和Netty的缘分">和Netty的缘分</h2><p>要聊能力前，先聊聊我和Netty的缘分。</p><p>最初接触Netty，是在一个和它完全不相关的项目。需求是要基于C++同事提供的网络通讯中间件写一个Java的网络通讯框架，有人实现了第一版，随后就由我接手了这框架。</p><p>在那个时候就有人告诉我，整个系统链路的性能瓶颈是在这个网络通讯框架上。</p><p>不过在当时也是才疏学浅，一开始定位方向就错了。我在那时候是觉得这框架卡在了服务端的处理上，使用了BIO而不是NIO的方式进行通讯，造成大量的线程空转，以致于拉低了吞吐量。</p><p>接着就开始研究NIO，也就自然而然地学习了Reactor，以及Netty。</p><p>得益于公司提供的极客时间企业会员，我在极客时间上把Netty的课程学了一遍，然后就慢慢了解了Netty的使用方式以及一些实现细节。</p><p>但学完之后却是完全没用上。直到后来做行情客户端、做公司业务协议的服务器时，才用上这部分知识。</p><p>最后，更是把Netty的能力拆分，用到了我自己写的交易框架上。</p><h2 id="FastThreadLocal快在哪？">FastThreadLocal快在哪？</h2><p>聊一个技术，总是得聊聊实现。</p><p>先说结论：<code>FastThreadLocal</code>比起JDK自带的<code>ThreadLocal</code>，少了一次Hash的计算，这就是它快的地方。</p><p>然后再来聊聊其中的实现。</p><p>JDK自带的<code>ThreadLocal</code>，为了做线程隔离，是在每个线程中都创建了一个Hash表。</p><p>这个Hash表的Key是<code>ThreadLocal</code>对象，Value是具体的值。</p><p>它的get()过程是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、Thread.currentThread()获取到当前线程</span><br><span class="line">2、从Thread对象的hash表中获取当前ThreadLocal对应的Object并返回</span><br></pre></td></tr></table></figure><p>在从hash表获取Object的过程中，不可避免会有一次hash计算。</p><p>而<code>FastThreadLocal</code>则不一样。Netty创建了一个<code>FastThreadLocalThread</code>，继承JDK的<code>Thread</code>类，使用一个<code>Object[]</code>替换了其中的Hash表。</p><p>核心思路就是，创建<code>FastThreadLocal</code>对象时，给其分配一个全局的ID（或者说index）。<br>这样的话，它的get()过程就可以优化为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、Thread.currentThread()获取到当前线程</span><br><span class="line">2、检查当前线程是不是FastThreadLocalThread，是的话走fastGet();否则走JDK原生的get()</span><br><span class="line">3、fastGet()会从FastThreadLocalThread的Object[]，通过创建FastThreadLocal获得的全局ID，直接用数组随机访问的方式获取值</span><br></pre></td></tr></table></figure><p>从结果上看，一个hash表取值的过程就优化为了数组随机访问，这就是最大的提升。</p><h2 id="带来了什么？">带来了什么？</h2><p>终于讲到重点了。<br>FastThreadLocal把访问线程本地变量的时间，从一次hash计算的时间优化到了一次数组随机访问的时间。从时间上看，大约是4-10ns的操作优化为了小于1ns的操作。</p><p>这带来了一些影响：<br>1、线程级别的资源，可以自行回收而不依赖JVM的GC。比如Netty中有一个工具类，存储着ArrayList和StringBuilder之类的对象，这些对象都存放于FastThreadLocal中。每次使用时，不需要new，而是从工具类中获取当前线程之前使用过的对象，使用完毕后手动清空这些对象即可。<br>2、在1的基础上，构建了Recycler，以及ObjectPool。这些是Netty中Pooled的Buffer的最核心实现。</p><p>也就是说，线程级别的资源访问成本变低了，而线程数量又可控的情况下，我们可以把某些资源每个线程都分配一个。</p><p>同时，在线程自己的视角，所有的资源都只被当前线程使用，线程安全，不需要做任何并发安全的编码，有效提升性能。</p><p>虽然运行时的内存占用上去了，但是不会有更多的垃圾对象产生，这就让JVM的GC压力变得特别小。</p><h2 id="感想">感想</h2><p>Netty不愧是Java编程的教科书，这个FastThreadLocal在我看来，几乎是Java编程思想的集大成之作，完美发挥了Java的长处，尽可能避免了其短处。<br>我虽然工作中已经有使用Netty，但我觉得更底层的细节其实我还是不太清楚。<br>不谈FastThreadLocal，Netty还有很多优秀的实现值得参考，比如它的’0拷贝’、堆外内存的池化和回收机制。这值得我继续深入研究。</p>]]></content>
    
    
    <summary type="html">在之前的工作中，每次和人吹牛逼我都会提到：Netty的FastThreadLocal是一个最基础最核心的技术。为什么这么说呢，这次我专门开一个篇章来聊聊。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="工作总结" scheme="https://tulancn.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>JNI调用的优化（第二版)</title>
    <link href="https://tulancn.github.io/2024/08/08/work/java/JNI%E8%B0%83%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88/"/>
    <id>https://tulancn.github.io/2024/08/08/work/java/JNI%E8%B0%83%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%88%E7%AC%AC%E4%BA%8C%E7%89%88/</id>
    <published>2024-08-08T02:28:13.000Z</published>
    <updated>2025-03-10T13:59:38.569Z</updated>
    
    <content type="html"><![CDATA[<p>源头是这篇博客：<a href="http://blog.hakugyokurou.net/?p=1758">http://blog.hakugyokurou.net/?p=1758</a></p><p>讲到了最正式的文档也就是bug系统中的一条记录：<a href="https://bugs.openjdk.org/browse/JDK-7013347">https://bugs.openjdk.org/browse/JDK-7013347</a></p><p>另外在stackoverflow上有人爆料了这个能力：<a href="https://stackoverflow.com/questions/36298111/is-it-possible-to-use-sun-misc-unsafe-to-call-c-functions-without-jni/36309652#36309652">https://stackoverflow.com/questions/36298111/is-it-possible-to-use-sun-misc-unsafe-to-call-c-functions-without-jni/36309652#36309652</a></p><p>处于验证的目的，我写了jmh对其进行测试。</p><p>不过测试结果倒是有提升，但也仅仅是20ns。</p><p>聊胜于无，就目前的场景（单次调用800ns）来看，并没有必要做这个优化。</p><p>带来的不稳定因素反而是更大的问题。</p>]]></content>
    
    
    <summary type="html">这一次验证了critical native调用。</summary>
    
    
    
    <category term="工作" scheme="https://tulancn.github.io/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
    <category term="性能调优" scheme="https://tulancn.github.io/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
</feed>
